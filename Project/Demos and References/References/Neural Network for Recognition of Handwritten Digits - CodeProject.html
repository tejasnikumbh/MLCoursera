<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html><head>
	<title>Neural Network for Recognition of Handwritten Digits - CodeProject</title> 
	<link type="text/css" rel="stylesheet" href="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Main.css">

	
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="Description" content="A convolutional neural network achieves 99.26% accuracy on a modified NIST database of hand-written digits.; Author: Mike O'Neill; Updated: 5 Dec 2006; Section: Libraries; Chapter: Platforms, Frameworks &amp; Libraries; Updated: 5 Dec 2006">
<meta name="Keywords" content="VC6, Win2K, WinXP, MFC, Dev, Advanced,Libraries,Platforms, Frameworks &amp; Libraries,Free source code, tutorials">
<meta name="Author" content="Mike O'Neill">
<meta name="Rating" content="General">
<meta name="Robots" content="index, follow, NOODP">
<meta name="Revisit-After" content="1 days">
<meta name="application-name" content="CodeProject">

<link rel="dns-prefetch" href="http://ajax.googleapis.com/"> 
<link rel="canonical" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi">


<link rel="alternate" type="application/rss+xml" title="CodeProject Latest articles - All Topics" href="http://www.codeproject.com/WebServices/ArticleRSS.aspx?cat=1">
<link rel="alternate" type="application/rss+xml" title="CodeProject Latest articles - MFC/C++" href="http://www.codeproject.com/WebServices/ArticleRSS.aspx?cat=2">
<link rel="alternate" type="application/rss+xml" title="CodeProject Latest articles - C#" href="http://www.codeproject.com/WebServices/ArticleRSS.aspx?cat=3">
<link rel="alternate" type="application/rss+xml" title="CodeProject Latest articles - VB.NET" href="http://www.codeproject.com/WebServices/ArticleRSS.aspx?cat=6">
<link rel="alternate" type="application/rss+xml" title="CodeProject Latest articles - Mobile" href="http://www.codeproject.com/WebServices/ArticleRSS.aspx?cat=18">
<link rel="alternate" type="application/rss+xml" title="CodeProject Latest articles - ASP.NET" href="http://www.codeproject.com/WebServices/ArticleRSS.aspx?cat=4">
<link rel="alternate" type="application/rss+xml" title="CodeProject Lounge Postings" href="http://www.codeproject.com/webservices/LoungeRSS.aspx">
<link rel="search" type="application/opensearchdescription+xml" title="CodeProject" href="http://www.codeproject.com/info/OpenSearch.xml">

	<!-- base href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" -->
	<link rel="icon" href="http://www.codeproject.com/favicon.ico" type="image/ico">
<link rel="shortcut icon" href="http://www.codeproject.com/favicon.ico" type="image/ico">
<link rel="apple-touch-icon" href="http://www.codeproject.com/images/FavIcon-Apple.png" type="image/png">
<script async="" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/cbgapi.loaded_1"></script><script async="" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/cbgapi.loaded_0"></script><script src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/widgets.js" id="twitter-wjs"></script><script gapi_processed="true" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/plusone.js" async="" type="text/javascript"></script><script src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/all.js" id="facebook-jssdk"></script><script type="text/javascript" async="" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/quant.js"></script><script type="text/javascript" language="Javascript">//<![CDATA[
function defrm () { /* thanks twitter */ document.write = ''; window.top.location = window.self.location;  setTimeout(function() { document.body.innerHTML = ''; }, 0);  window.self.onload = function(evt) { document.body.innerHTML = ''; }; }if (window.top !== window.self) {  try {  if (window.top.location.host) { /* will throw */ } else { defrm(); /* chrome */ }  } catch (ex) { defrm(); /* everyone else */ } }if (typeof(DemoUrl)!='undefined')   document.write(unescape('%3Cme')+'ta http'+'-equiv="re'+'fresh"                  con'+'tent="1;url='+DemoUrl+unescape('"%3CE'));
function _dmBootstrap(file) { var _dma = document.createElement('script');  _dma.type = 'text/javascript'; _dma.async = true;  _dma.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + file; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(_dma);}
function _dmFollowup(file) { if (typeof DMAds === 'undefined')  _dmBootstrap('cdn2.developermedia.com/a.min.js?dt=2.6.1309023.2');}
(function () { _dmBootstrap('cdn1.developermedia.com/a.min.js?dt=2.6.1309023.2'); setTimeout(_dmFollowup, 2000);})();

//]]>
</script><script src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/a.js" async="" type="text/javascript"></script>

	




<script type="text/javascript">
	var _gaq = _gaq || [];
	_gaq.push(['_setAccount', 'UA-1735123-1']);
	_gaq.push(['_trackPageview']);
	_gaq.push(['_setDomainName', 'www.codeproject.com']);
	_gaq.push(['_setSessionTimeout', '1200']); 

	(function () {
		var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ga);
	})(); 
</script><script src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ga.js" async="" type="text/javascript"></script>


<style type="text/css">.fb_hidden{position:absolute;top:-10000px;z-index:10001}
.fb_invisible{display:none}
.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}
.fb_reset > div{overflow:hidden}
.fb_link img{border:none}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}
.fb_dialog_advanced{padding:10px;-moz-border-radius:8px;-webkit-border-radius:8px;border-radius:8px}
.fb_dialog_content{background:#fff;color:#333}
.fb_dialog_close_icon{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;_background-image:url(http://static.ak.fbcdn.net/rsrc.php/v2/yL/r/s816eWC-2sl.gif);cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px;top:8px\9;right:7px\9}
.fb_dialog_mobile .fb_dialog_close_icon{top:5px;left:5px;right:auto}
.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}
.fb_dialog_close_icon:hover{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent;_background-image:url(http://static.ak.fbcdn.net/rsrc.php/v2/yL/r/s816eWC-2sl.gif)}
.fb_dialog_close_icon:active{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent;_background-image:url(http://static.ak.fbcdn.net/rsrc.php/v2/yL/r/s816eWC-2sl.gif)}
.fb_dialog_loader{background-color:#f2f2f2;border:1px solid #606060;font-size:24px;padding:20px}
.fb_dialog_top_left,
.fb_dialog_top_right,
.fb_dialog_bottom_left,
.fb_dialog_bottom_right{height:10px;width:10px;overflow:hidden;position:absolute}
.fb_dialog_top_left{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 0;left:-10px;top:-10px}
.fb_dialog_top_right{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 -10px;right:-10px;top:-10px}
.fb_dialog_bottom_left{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 -20px;bottom:-10px;left:-10px}
.fb_dialog_bottom_right{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/ye/r/8YeTNIlTZjm.png) no-repeat 0 -30px;right:-10px;bottom:-10px}
.fb_dialog_vert_left,
.fb_dialog_vert_right,
.fb_dialog_horiz_top,
.fb_dialog_horiz_bottom{position:absolute;background:#525252;filter:alpha(opacity=70);opacity:.7}
.fb_dialog_vert_left,
.fb_dialog_vert_right{width:10px;height:100%}
.fb_dialog_vert_left{margin-left:-10px}
.fb_dialog_vert_right{right:0;margin-right:-10px}
.fb_dialog_horiz_top,
.fb_dialog_horiz_bottom{width:100%;height:10px}
.fb_dialog_horiz_top{margin-top:-10px}
.fb_dialog_horiz_bottom{bottom:0;margin-bottom:-10px}
.fb_dialog_iframe{line-height:0}
.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #3b5998;color:#fff;font-size:14px;font-weight:bold;margin:0}
.fb_dialog_content .dialog_title > span{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/yd/r/Cou7n-nqK52.gif)
no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}
body.fb_hidden{-webkit-transform:none;height:100%;margin:0;left:-10000px;overflow:visible;position:absolute;top:-10000px;width:100%
}
.fb_dialog.fb_dialog_mobile.loading{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/ya/r/3rhSv5V8j3o.gif)
white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}
.fb_dialog.fb_dialog_mobile.loading.centered{max-height:590px;min-height:590px;max-width:500px;min-width:500px}
#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .45);position:absolute;left:0;top:0;width:100%;min-height:100%;z-index:10000}
#fb-root #fb_dialog_ipad_overlay.hidden{display:none}
.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}
.fb_dialog_content .dialog_header{-webkit-box-shadow:white 0 1px 1px -1px inset;background:-webkit-gradient(linear, 0 0, 0 100%, from(#738ABA), to(#2C4987));border-bottom:1px solid;border-color:#1d4088;color:#fff;font:14px Helvetica, sans-serif;font-weight:bold;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}
.fb_dialog_content .dialog_header table{-webkit-font-smoothing:subpixel-antialiased;height:43px;width:100%
}
.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px
}
.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px
}
.fb_dialog_content .touchable_button{background:-webkit-gradient(linear, 0 0, 0 100%, from(#4966A6),
color-stop(0.5, #355492), to(#2A4887));border:1px solid #29447e;-webkit-background-clip:padding-box;-webkit-border-radius:3px;-webkit-box-shadow:rgba(0, 0, 0, .117188) 0 1px 1px inset,
rgba(255, 255, 255, .167969) 0 1px 0;display:inline-block;margin-top:3px;max-width:85px;line-height:18px;padding:4px 12px;position:relative}
.fb_dialog_content .dialog_header .touchable_button input{border:none;background:none;color:#fff;font:12px Helvetica, sans-serif;font-weight:bold;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}
.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}
.fb_dialog_content .dialog_content{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #555;border-bottom:0;border-top:0;height:150px}
.fb_dialog_content .dialog_footer{background:#f2f2f2;border:1px solid #555;border-top-color:#ccc;height:40px}
#fb_dialog_loader_close{float:left}
.fb_dialog.fb_dialog_mobile .fb_dialog_close_button{text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}
.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}
.fb_iframe_widget{display:inline-block;position:relative}
.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}
.fb_iframe_widget iframe{position:absolute}
.fb_iframe_widget_lift{z-index:1}
.fb_hide_iframes iframe{position:relative;left:-10000px}
.fb_iframe_widget_loader{position:relative;display:inline-block}
.fb_iframe_widget_fluid{display:inline}
.fb_iframe_widget_fluid span{width:100%}
.fb_iframe_widget_loader iframe{min-height:32px;z-index:2;zoom:1}
.fb_iframe_widget_loader .FB_Loader{background:url(http://static.ak.fbcdn.net/rsrc.php/v2/y9/r/jKEcVPZFk-2.gif) no-repeat;height:32px;width:32px;margin-left:-16px;position:absolute;left:50%;z-index:4}
.fb_connect_bar_container div,
.fb_connect_bar_container span,
.fb_connect_bar_container a,
.fb_connect_bar_container img,
.fb_connect_bar_container strong{background:none;border-spacing:0;border:0;direction:ltr;font-style:normal;font-variant:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal;vertical-align:baseline}
.fb_connect_bar_container{position:fixed;left:0 !important;right:0 !important;height:42px !important;padding:0 25px !important;margin:0 !important;vertical-align:middle !important;border-bottom:1px solid #333 !important;background:#3b5998 !important;z-index:99999999 !important;overflow:hidden !important}
.fb_connect_bar_container_ie6{position:absolute;top:expression(document.compatMode=="CSS1Compat"? document.documentElement.scrollTop+"px":body.scrollTop+"px")}
.fb_connect_bar{position:relative;margin:auto;height:100%;width:100%;padding:6px 0 0 0 !important;background:none;color:#fff !important;font-family:"lucida grande", tahoma, verdana, arial, sans-serif !important;font-size:13px !important;font-style:normal !important;font-variant:normal !important;font-weight:normal !important;letter-spacing:normal !important;line-height:1 !important;text-decoration:none !important;text-indent:0 !important;text-shadow:none !important;text-transform:none !important;white-space:normal !important;word-spacing:normal !important}
.fb_connect_bar a:hover{color:#fff}
.fb_connect_bar .fb_profile img{height:30px;width:30px;vertical-align:middle;margin:0 6px 5px 0}
.fb_connect_bar div a,
.fb_connect_bar span,
.fb_connect_bar span a{color:#bac6da;font-size:11px;text-decoration:none}
.fb_connect_bar .fb_buttons{float:right;margin-top:7px}
.fb_edge_widget_with_comment{position:relative;*z-index:1000}
.fb_edge_widget_with_comment span.fb_edge_comment_widget{position:absolute}
.fb_edge_widget_with_comment span.fb_send_button_form_widget{z-index:1}
.fb_edge_widget_with_comment span.fb_send_button_form_widget .FB_Loader{left:0;top:1px;margin-top:6px;margin-left:0;background-position:50% 50%;background-color:#fff;height:150px;width:394px;border:1px #666 solid;border-bottom:2px solid #283e6c;z-index:1}
.fb_edge_widget_with_comment span.fb_send_button_form_widget.dark .FB_Loader{background-color:#000;border-bottom:2px solid #ccc}
.fb_edge_widget_with_comment span.fb_send_button_form_widget.siderender
.FB_Loader{margin-top:0}
.fbpluginrecommendationsbarleft,
.fbpluginrecommendationsbarright{position:fixed !important;bottom:0;z-index:999}
.fbpluginrecommendationsbarleft{left:10px}
.fbpluginrecommendationsbarright{right:10px}</style></head>	

<body data-twttr-rendered="true" class="firefox firefox23">

<a href="#Main"><img alt="Click here to Skip to main content" class="access-link" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/t_002.gif"></a>





<div class="page-background">

	
	

	

	<table id="ctl00_Bn" style="width:100%;height:135px" class="banner fixed" cellpadding="0" cellspacing="0">
	<tbody><tr valign="bottom">
		<td class="blank-background" style="height:31px">&nbsp;</td>
		<td class="blank-background" rowspan="3" style="width:250px;height:135px"><a href="http://www.codeproject.com/"><img id="ctl00_Logo" tabindex="1" title="CodeProject" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/logo250x135.gif" alt="Home" style="height:135px;width:250px;border-width:0px;"></a></td>
		<td class="blank-background align-right" style="width:728px;height:31px">

<div class="container memberbar clearfix">

	<div id="ctl00_MemberMenu_GenInfo" class="float-left">10,104,583 members (43,817 online) &nbsp; &nbsp; </div>

	<div class="float-left">
		
	</div>

	<div class="float-right">

		<span id="ctl00_MemberMenu_CodeProjectTV" class="tooltip" style="margin-right:15px;">
			<div class="speech-bubble-container-up" style="width:180px;line-height:20px">
				<div class="speech-bubble-up">
					<b><a href="http://codeproject.tv/">Visit CodeProject.TV</a><br>
					<a id="ctl00_MemberMenu_DiscussCPTV" href="http://www.codeproject.com/Forums/1829610/CodeProject-TV.aspx">Discuss CodeProject.TV</a></b>
				</div>
				<div class="speech-bubble-pointer-up">
					<div class="speech-bubble-pointer-up-inner"></div>
				</div>
			</div>
		</span>

		

		

		

			<script type="text/javascript">//<!--
			function doSubmit(secure)
			{
				if (secure)
					document.subForm.action = "https://www.codeproject.com/script/Membership/LogOn.aspx?rp=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi"
				else
					document.subForm.action = "https://www.codeproject.com/script/Membership/LogOn.aspx?rp=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi"
				document.subForm.submit();
				return true;
			}//-->
			</script>

			<a name="SignUp"></a>
			<span class="member-signin tooltip">
				<span><a href="https://www.codeproject.com/script/Membership/LogOn.aspx?rp=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi">Sign in</a></span>

				<div class="tooltip-flyout">
					<form name="subForm" id="subForm" action="https://www.codeproject.com/script/Membership/LogOn.aspx?rp=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi" method="post" class="tight">

						
						<input id="FormName" name="FormName" value="MenuBarForm" type="hidden">

						<div>Email</div>
						<div><input class="small-text" name="Email" id="Email" type="email"></div>
						<div>Password</div>
						<div><input class="small-text" name="Password" id="Password" type="password"></div>
						<div class="action">
							<script type="text/javascript">
function Join(){document.location.href='http://www.codeproject.com/script/Membership/Modify.aspx';return false;}
document.write('<input type="button" class="create" onclick="return Join();" value="Join"');
document.write('<input type="hidden" name="fld_quicksign" value="true" />');
</script><input class="create" onclick="return Join();" value="Join" <input="" name="fld_quicksign" type="button">
							<input value="Sign in" class="signin" onclick="return doSubmit(false);" type="submit">
						</div>

						<div class="container">
							
							&nbsp;
							<a id="ctl00_MemberMenu_SendPassword" class="forgot float-right" href="http://www.codeproject.com/script/Membership/SendPassword.aspx?rp=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi">Forgot your password?</a>
						</div>
					</form>

					<hr class="divider-dark">

					Sign in using <a class="oauth" title="Sign in using Facebook" href="http://www.codeproject.com/script/Membership/OAuthLogOn.aspx?auth=Facebook"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/facebook.png" style="vertical-align:middle;padding-right:3px;border:0;"></a>
<a class="oauth" title="Sign in using Google" href="http://www.codeproject.com/script/Membership/OAuthLogOn.aspx?auth=Google"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/google-plus.png" style="vertical-align:middle;padding-right:3px;border:0;"></a>
<a class="oauth" title="Sign in using Linkedin" href="http://www.codeproject.com/script/Membership/OAuthLogOn.aspx?auth=LinkedIn"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/linkedin.png" style="vertical-align:middle;padding-right:3px;border:0;"></a>

				</div>
		
	</span></div>
</div></td>
		<td class="blank-background" style="height:31px">&nbsp;</td>
	</tr>
	<tr valign="middle">
		<td class="theme1-background" style="height:94px">&nbsp;</td>
		<td class="theme1-background ad"><div class="msg-728x90" data-format="728x90" data-type="ad" data-publisher="lqm.codeproject.site" data-zone="ros" data-tags="VC6, Win2K, WinXP, MFC, Dev, Advanced,rating4.5"><iframe id="dmad1" allowtransparency="false" style="z-index:10" marginwidth="0" marginheight="0" frameborder="0" height="90" scrolling="no" width="728"></iframe></div></td>
		<td class="theme1-background" style="height:94px">&nbsp;</td>
	</tr>
	<tr valign="top">
		<td style="height: 10px;"></td>
		<td style="height: 10px;" class="blank-background"></td>
		<td style="height: 10px;"></td>
	</tr>
</tbody></table>


	<a href="#Main"><img alt="Click here to Skip to main content" class="access-link" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/t_002.gif"></a>

	
	<div id="ctl00_TPR" class="sub-headerbar fixed">
	<table class="extended" cellpadding="0" cellspacing="0"><tbody><tr><td nowrap="nowrap">
		

<div class="navbar clearfix">
<ul class="navmenu openable">

<li><a id="ctl00_TopNavBar_Home" href="http://www.codeproject.com/">home</a>


</li><li class=""><a id="ctl00_TopNavBar_Art" class="down selected" href="http://www.codeproject.com/script/Articles/Latest.aspx">articles</a>

	<ul>
		<li class=""><a id="ctl00_TopNavBar_ArtTopicList" class="fly" onmouseover="navBarMenu.ShowMap(this, 'ctl00_TopNavBar_MapFlyout');" href="http://www.codeproject.com/script/Content/SiteMap.aspx">Chapters and Sections<span class="has-submenu">&gt;</span></a><ul id="ctl00_TopNavBar_MapFlyout">
			<li>
				<div id="siteMap">
					<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/animated.gif" alt="loading" style="margin:150px;width:100px;height:100px;">
				</div>
			</li>
			</ul>
		</li>
		<li><a id="ctl00_TopNavBar_ArtSearch" class="fly break" href="http://www.codeproject.com/search.aspx">Search</a></li>
		<li><a id="ctl00_TopNavBar_ArtLatestArts" class="fly" href="http://www.codeproject.com/script/Articles/Latest.aspx?at=1,3,7">Latest Articles</a></li>
		<li><a id="ctl00_TopNavBar_ArtLatestTips" class="fly" href="http://www.codeproject.com/script/Articles/Latest.aspx?at=6">Latest Tips/Tricks</a></li>
		<li><a id="ctl00_TopNavBar_ArtTop" class="fly" href="http://www.codeproject.com/script/Articles/TopArticles.aspx?ta_so=5">Top Articles</a></li>
		<li><a id="ctl00_TopNavBar_ArtBeginner" class="fly" href="http://www.codeproject.com/search.aspx?aidlst=152&amp;sa_us=True">Beginner Articles</a></li>
		<li><a id="ctl00_TopNavBar_ArtBlogArticles" class="fly break" href="http://www.codeproject.com/script/Articles/BlogArticleList.aspx">Technical Blogs</a></li>
		<li><a id="ctl00_TopNavBar_ArtGuide" class="fly" href="http://www.codeproject.com/info/Submit.aspx">Posting/Update Guidelines</a></li>
		<li><a id="ctl00_TopNavBar_ArtHelpForum" class="fly" href="http://www.codeproject.com/Forums/1641/Article-Writing.aspx">Article Help Forum</a></li>
		<li><a id="ctl00_TopNavBar_ArtCompetition" class="fly break" href="http://www.codeproject.com/script/Awards/CurrentCompetitions.aspx?cmpTpId=1">Article Competition</a></li>
		<li><a id="ctl00_TopNavBar_ArtPostArticle" class="fly highlight1" href="http://www.codeproject.com/script/Articles/Submit.aspx">
			<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/write-gr.png" height="13px" width="19px"> Submit an article or tip
			</a></li>
		<li><a id="ctl00_TopNavBar_ArtPostBlog" class="fly highlight2" href="http://www.codeproject.com/script/Articles/BlogFeed.aspx">
			<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/write-or.png" height="13px" width="19px"> Post your Blog
			</a></li>		<li class="last"></li>
	</ul>

</li>



<li class=""><a id="ctl00_TopNavBar_Answers" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active">quick answers</a>
	<ul>
		<li id="ctl00_TopNavBar_AQL"><a id="ctl00_TopNavBar_ArticleQuestion" class="fly highlight1" href="#_comments">
			<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/write-gr.png" height="13px" width="19px"> Ask a Question about this 
			article</a>
		</li>

		<li><a id="ctl00_TopNavBar_QAAsk" class="fly highlight2" href="http://www.codeproject.com/Questions/ask.aspx"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/write-or.png" height="13px" width="19px"> Ask a Question</a></li>

		
		<li><a id="ctl00_TopNavBar_QAUnanswered" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=unanswered">View Unanswered Questions</a></li>
		<li><a id="ctl00_TopNavBar_QALatest" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active">View All Questions...</a></li>
		
				<li><a id="ctl00_TopNavBar_QATR_ctl00_Tag" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active&amp;alltags=true&amp;tags=81" style="padding-left:30px">C# questions</a></li>
			
				<li><a id="ctl00_TopNavBar_QATR_ctl01_Tag" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active&amp;alltags=true&amp;tags=85" style="padding-left:30px">ASP.NET questions</a></li>
			
				<li><a id="ctl00_TopNavBar_QATR_ctl02_Tag" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active&amp;alltags=true&amp;tags=842" style="padding-left:30px">VB.NET questions</a></li>
			
				<li><a id="ctl00_TopNavBar_QATR_ctl03_Tag" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active&amp;alltags=true&amp;tags=308" style="padding-left:30px">C#4.0 questions</a></li>
			
				<li><a id="ctl00_TopNavBar_QATR_ctl04_Tag" class="fly" href="http://www.codeproject.com/script/Answers/List.aspx?tab=active&amp;alltags=true&amp;tags=78" style="padding-left:30px">C++ questions</a></li>
			
		<li class="last"></li>
	</ul>

</li>



<li class=""><a id="ctl00_TopNavBar_Forums" href="http://www.codeproject.com/script/Forums/List.aspx">discussions</a>

	<ul>
		<li><a id="ctl00_TopNavBar_MessageBoardsAll" class="fly" href="http://www.codeproject.com/script/Forums/List.aspx">All Message Boards...</a></li>
		<li class=""><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1580997/Application-Lifecycle.aspx">Application Lifecycle<span class="has-submenu">&gt;</span></a>
<ul class="openable"><li><a class="fly" href="http://www.codeproject.com/Forums/1533717/Running-a-Business.aspx">Running a Business</a></li>
<li><a class="fly" href="http://www.codeproject.com/Forums/1533716/Sales-Marketing.aspx">Sales / Marketing</a></li>
<li><a class="fly" href="http://www.codeproject.com/Forums/1651/Collaboration-Beta-Testing.aspx">Collaboration / Beta Testing</a></li>
<li><a class="fly" href="http://www.codeproject.com/Forums/3304/Work-Training-Issues.aspx">Work &amp; Training Issues</a></li>
</ul></li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/369270/Design-and-Architecture.aspx">Design and Architecture</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/12076/ASP-NET.aspx">ASP.NET</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1580226/JavaScript.aspx">JavaScript</a>
</li>
<li class=""><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1647/C-Cplusplus-MFC.aspx">C / C++ / MFC<span class="has-submenu">&gt;</span></a>
<ul class="openable"><li><a class="fly" href="http://www.codeproject.com/Forums/4486/ATL-WTL-STL.aspx">ATL /  WTL / STL</a></li>
<li><a class="fly" href="http://www.codeproject.com/Forums/3785/Managed-Cplusplus-CLI.aspx">Managed C++/CLI</a></li>
</ul></li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1827459/Adobe-Technologies.aspx">Adobe Technologies</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1649/Csharp.aspx">C#</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1627782/Free-Tools.aspx">Free Tools</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1827460/Objective-C.aspx">Objective-C</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1832431/Ruby-On-Rails.aspx">Ruby On Rails</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1725/Database.aspx">Database</a>
</li>
<li class=""><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/186301/Hardware-Devices.aspx">Hardware &amp; Devices<span class="has-submenu">&gt;</span></a>
<ul class="openable"><li><a class="fly" href="http://www.codeproject.com/Forums/1644/System-Admin.aspx">System Admin</a></li>
</ul></li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1606152/Hosting-and-Servers.aspx">Hosting and Servers</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1643/Java.aspx">Java</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1650/NET-Framework.aspx">.NET Framework</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/13695/Mobile.aspx">Mobile</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1653293/VS-2012-2013-NET-4-5-1.aspx">VS 2012/2013 &amp; .NET 4.5.1</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1540733/Sharepoint.aspx">Sharepoint</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1004257/Silverlight-WPF.aspx">Silverlight / WPF</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1646/Visual-Basic.aspx">Visual Basic</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1640/Web-Development.aspx">Web Development</a>
</li>
<li><a class="fly" style="padding-left:30px" href="http://www.codeproject.com/Forums/1645/Site-Bugs-Suggestions.aspx">Site Bugs / Suggestions</a>
</li>

		<li class="last"></li>
	</ul>

</li>




<li class=""><a id="ctl00_TopNavBar_Features" href="http://www.codeproject.com/Feature/">features</a>

	<ul>
		<li><a id="ctl00_TopNavBar_CPTV" class="fly highlight1" href="http://codeproject.tv/">
			<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/CPTV-24.png" alt="CP.TV" style="vertical-align:text-bottom" height="24px" width="24px">
				CodeProject.TV</a></li>
		<li><a id="ctl00_TopNavBar_Catalog" class="fly" href="http://www.codeproject.com/script/Catalog/List.aspx">Component &amp; Service Catalog</a></li>
		<li><a id="ctl00_TopNavBar_Comps" class="fly" href="http://www.codeproject.com/script/Awards/CurrentCompetitions.aspx?cmpTpId=1&amp;awsac=true">Competitions</a></li>
		<li><a id="ctl00_TopNavBar_News" class="fly" href="http://www.codeproject.com/script/News/List.aspx">News</a></li>
		<li><a id="ctl00_TopNavBar_Insider" class="fly" href="http://www.codeproject.com/Feature/Insider/">The Insider Newsletter</a></li>
		<li><a id="ctl00_TopNavBar_Newsletters" class="fly" href="http://www.codeproject.com/script/Mailouts/Archive.aspx?mtpid=1">Newsletter archive</a></li>
		<li><a id="ctl00_TopNavBar_Surveys" class="fly" href="http://www.codeproject.com/script/Surveys/List.aspx">Surveys</a></li>
		<li><a id="ctl00_TopNavBar_Showcase" class="fly" href="http://www.codeproject.com/KB/showcase/">Product Showcase</a></li>
		<li><a id="ctl00_TopNavBar_Research" class="fly" href="http://www.codeproject.com/script/ResearchLibrary/Index.aspx">Research Library</a></li>

		
		<li><a id="ctl00_TopNavBar_Stuff" class="fly" href="http://www.codeproject.com/Info/Stuff.aspx">CodeProject Stuff</a></li>
		<li class="last"></li>
	</ul>

</li>


<li class=""><a id="ctl00_TopNavBar_Lounge" href="http://www.codeproject.com/Lounge.aspx">community</a>

	<ul>
		<li><a id="ctl00_TopNavBar_InsiderLnk" class="fly" href="http://www.codeproject.com/Insider.aspx">The Insider News</a></li>
		<li><a id="ctl00_TopNavBar_LoungeLnk" class="fly" href="http://www.codeproject.com/Lounge.aspx">The Lounge &nbsp;</a></li>
		<li><a id="ctl00_TopNavBar_WeirdWonderful" class="fly" href="http://www.codeproject.com/feature/weirdandwonderful.aspx">The Weird &amp; The Wonderful</a></li>
		<li><a id="ctl00_TopNavBar_SoapBoxLnk" class="fly" href="http://www.codeproject.com/Forums/1536756/The-Soapbox.aspx">The Soapbox</a></li>
		<li><a id="ctl00_TopNavBar_PRLnk" class="fly break" href="http://www.codeproject.com/Forums/1738007/Press-Releases.aspx">Press Releases</a></li>
		<li><a id="ctl00_TopNavBar_WhosWho" class="fly" href="http://www.codeproject.com/script/Membership/Profiles.aspx">Who's Who</a></li>
		<li><a id="ctl00_TopNavBar_MVPs" class="fly" href="http://www.codeproject.com/script/Awards/MVPWinners.aspx">Most Valuable Professionals</a></li>
		<li><a id="ctl00_TopNavBar_Companies" class="fly break" href="http://www.codeproject.com/script/Membership/Profiles.aspx?mgtid=1&amp;mgm=True">Company Listings</a></li>

		
		<li class=""><a class="fly" href="http://www.codeproject.com/Forums/1580229/Hindi.aspx">Non-English Language
			<span class="has-submenu">&gt;</span></a>
		<ul>
		<li><a class="fly" href="http://www.codeproject.com/Forums/1580229/Hindi.aspx">General Indian Topics</a></li>
		<li><a class="fly" href="http://www.codeproject.com/Forums/1580230/Chinese.aspx">General Chinese Topics</a></li>
		</ul>
		</li><li class="last"></li>
		
	</ul>

</li>


<li class="" style="margin-left:20px"><a id="ctl00_TopNavBar_Help" href="http://www.codeproject.com/KB/FAQs/">help</a>

	<ul>
		<li><a id="ctl00_TopNavBar_HelpWhatIs" class="fly" href="http://www.codeproject.com/info/guide.aspx">What is 'CodeProject'?</a></li>
		<li><a id="ctl00_TopNavBar_HelpGeneral" class="fly break" href="http://www.codeproject.com/KB/FAQs/">General FAQ</a></li>
		<li><a id="ctl00_TopNavBar_HelpPostQuestion" class="fly break highlight1" href="http://www.codeproject.com/Questions/ask.aspx">Ask a Question</a></li>
		<li><a id="ctl00_TopNavBar_HelpBugs" class="fly" href="http://www.codeproject.com/Forums/1645/Site-Bugs-Suggestions.aspx">Bugs and Suggestions</a></li>
		<li><a id="ctl00_TopNavBar_HelpArticles" class="fly" href="http://www.codeproject.com/Forums/1641/Article-Writing.aspx">Article Help Forum</a></li>
		<li><a id="ctl00_TopNavBar_HelpSiteMap" class="fly" href="http://www.codeproject.com/script/Content/SiteMap.aspx">Site Map</a></li>
		<li><a id="ctl00_TopNavBar_HelpAdvertise" class="fly" href="http://developermedia.com/">Advertise with us</a></li>
		<li><a id="ctl00_TopNavBar_HelpJobs" class="fly" href="http://www.codeproject.com/info/Jobs/">Employment Opportunities</a></li>
		<li><a id="ctl00_TopNavBar_HelpAboutUs" class="fly" href="http://www.codeproject.com/info/about.aspx">About Us</a></li>
		<li class="last"></li>
	</ul>

</li>

</ul>

</div>
	</td><td align="right">
		

<div class="searchbar">

<form method="get" action="/search.aspx" name="Search" class="tight">


<table class="search" border="0" cellpadding="0" cellspacing="0"><tbody><tr><td><input tabindex="2" class="search  subdue" id="sb_tb" value="Search for articles, questions, tips" name="q"></td><td><input src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/search.gif" type="image"></td></tr></tbody></table>

<div class="hover-container">
	<div style="display: none;" id="SearchFilter" class="search-advanced small-text align-left">
	<b>Search within:<br></b>
		
		<input id="sb_kw" name="sbo" value="kw" checked="checked" type="radio"><label for="sb_kw">Articles</label><br>
<input id="sb_vkw" name="sbo" value="vkw" type="radio"><label for="sb_vkw">Videos</label><br>
<input id="sb_qa" name="sbo" value="qa" type="radio"><label for="sb_qa">Quick Answers</label><br>
<input id="sb_fm" name="sbo" value="fm" type="radio"><label for="sb_fm">Messages</label><br>
<input id="sb_ctlk" name="sbo" value="ctlk" type="radio"><label for="sb_ctlk">Product Catalog</label><br>


		
		
	</div>
</div>
</form>

</div>
	</td></tr></tbody></table>
	<div class="sub-headerbar-divider"></div>
	</div>
	

	<div id="A" class="container-content-wrap fixed"> 

	<div itemscope="" itemtype="http://schema.org/Article" class="container-content"> 

		<div class="clearfix">
			<div class="float-left container-breadcrumb">
				<div><a href="http://www.codeproject.com/script/Content/SiteMap.aspx">Articles</a> » <a href="http://www.codeproject.com/Chapters/8/Platforms-Frameworks-Libraries.aspx">Platforms, Frameworks &amp; Libraries</a> » <a href="http://www.codeproject.com/KB/library/"><span itemprop="articleSection">Libraries</span></a> » <a href="http://www.codeproject.com/KB/library/#General">General</a></div>
			</div>

			<div class="align-left float-right padded-top">
				


 
&nbsp;










			</div>

			<div class="float-right container-breadcrumb article-nav">
				
<div class="">


<a id="ctl00_PrevNext_NextLink" title="Next" href="http://www.codeproject.com/script/Articles/PrevNextLookup.aspx?aid=16650&amp;at=1&amp;secId=119" style="margin-left:5px">Next</a>
<img id="ctl00_PrevNext_NextImg" title="Next" rel="nofollow" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/arrow-right.png" style="border-width:0px;vertical-align:bottom;">
</div>
			</div>

			<div class="align-right float-left">
				
			</div>
		</div>

		<table class="extended container-article-parts" cellpadding="0" cellspacing="0"><tbody><tr valign="top">
		<td>

			<div id="ctl00_Nav" class="container-article-tabs">
				<div class="tabs">
					

<div class="">
	<div class="selected">Article</div><div class="unselected"><a href="http://www.codeproject.com/script/Articles/ViewDownloads.aspx?aid=16650">Browse Code</a></div><div class="unselected"><a href="http://www.codeproject.com/script/Articles/Statistics.aspx?aid=16650">Stats</a></div><div class="unselected"><a href="http://www.codeproject.com/script/Articles/ListVersions.aspx?aid=16650">Revisions</a></div><div class="unselected"><a href="http://www.codeproject.com/script/Articles/ListAlternatives.aspx?aid=16650">Alternatives</a></div>
</div>	


					<!-- anchorLink used to auto-link to comments at end of article -->
					<div class="unselected"><a href="http://www.codeproject.com/MasterPages/#_comments" id="ctl00_CommentLink" class="anchorLink">Comments &amp; 
						Discussions <span id="ctl00_CmtCnt">(202)</span></a>
					</div>
				</div>

				
			</div>

		</td>
		<td>
			
			<div id="AT" class="container-article  fixed"> 
				<div class="article">

					
					 
					<div class="header">

					<a name="Main"></a>

					
					<a name="_articleTop" id="_articleTop"></a>
					<div class="title">
					
					
					<h1 id="ctl00_ArticleTitle" itemprop="name">Neural Network for Recognition of Handwritten Digits</h1> 
					</div>

					
					<div class="entry">
						By <span class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=208786" rel="author"><span itemprop="author" itemscope="" itemtype="http://schema.org/Person"><span itemprop="name">Mike O'Neill</span></span></a></span>, 
						<span class="date" itemprop="dateModified" content="2006-12-05 14:38:00">
							5 Dec 2006</span>
			
									
					</div>

					
					<!-- Tables. Yes. I'm weak -->
					<table class="container-rating"><tbody><tr><td>
						<div id="ctl00_CurRat" class="tooltip anchorLink" style="cursor:pointer" onclick="scrollToAnchor('#_rating');" name="CurRat_16650">
				
							

<table class="small-text" itemprop="aggregateRating" itemscope="" itemtype="http://schema.org/AggregateRating" cellpadding="0" cellspacing="0"> 
<tbody><tr>
	
	<td class="nowrap">

		
			<meta itemprop="bestRating" content="5"> 
			<meta itemprop="worstRating" content="1">
		

		<span id="ctl00_ArticleRating_VI">
		<div class="nowrap rating-stars-large" style="height:19px;width:139px;position:relative;">
	<div class="clipped align-left float-left" style="height:19px;width:138px;">
		<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/stars-fill-lg.png" style="border-width:0px;">
	</div><div class="clipped" style="height:19px;width:1px;position:relative;">
		<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/stars-empty-lg.png" style="border-width:0px;position:absolute;top:0;right:0;">
	</div>
</div>
		</span>

		
	</td>
	
	<td id="ctl00_ArticleRating_VR" class="nowrap">
		&nbsp;
		<span id="ctl00_ArticleRating_VotesR">&nbsp;<span itemprop="ratingValue" class="rating">4.97</span> (<span itemprop="ratingCount" class="count">163</span> votes)</span>
		
	</td>

</tr>

</tbody></table>


							<div id="ctl00_RB" class="speech-bubble-container-up">
								<div class="speech-bubble-up" style="width:150px !important">
									            
<div>
<table class="feature" title="Voting Distribution. Recent data only" cellpadding="0" cellspacing="0" height="50px" width="100%"><tbody><tr class="chart-row"><td class="chart-column rating-ignore-vote" title="Outside deviation limits - not included in score."><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/pollcol.gif" alt="3 votes, 1.9%" title="3 votes, 1.9%" border="0px" height="1px" width="20pxpx"><br><span title="3 votes">1</span></td>
<td class="chart-column rating-ignore-vote" title="Outside deviation limits - not included in score."><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/t_002.gif" alt="" title="" border="0px" height="1px" width="20pxpx"><br><span title="0 votes">2</span></td>
<td class="chart-column rating-ignore-vote" title="Outside deviation limits - not included in score."><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/t_002.gif" alt="" title="" border="0px" height="1px" width="20pxpx"><br><span title="0 votes">3</span></td>
<td class="chart-column"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/pollcol.gif" alt="4 votes, 2.5%" title="4 votes, 2.5%" border="0px" height="1px" width="20pxpx"><br><span title="4 votes">4</span></td>
<td class="chart-column"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/pollcol.gif" alt="154 votes, 95.7%" title="154 votes, 95.7%" border="0px" height="50px" width="20pxpx"><br><span title="154 votes">5</span></td>
</tr></tbody></table><div class="small-text align-center">4.97/5 - 163 votes</div><div class="small-text align-center subdue">3 removed</div><div class="small-text align-center subdue">μ 4.94, σ<sub>a</sub> 0.99 [<a href="http://www.codeproject.com/KB/FAQs/RatingReputationFAQ.aspx#noisefilter">?</a>]</div>
</div>
								</div>
								<div class="speech-bubble-pointer-up">
									<div class="speech-bubble-pointer-up-inner"></div>
								</div>
							</div>
						</div>
					</td>
					<td>
						&nbsp;
						
					</td></tr></tbody></table>

					</div>
					
					

					
					<div class="prize-winner">
<div><img id="ctl00_ArticleAwards_AR_ctl01_AI" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/award24.gif" style="border-width:0px;" align="absmiddle"> Prize winner in Competition 
"MFC/C++ Nov 2006" <i></i></div>
</div>

					
					
					

						
					

					<form name="aspnetForm" method="post" action="/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" id="aspnetForm" style="margin:0;padding:0">
<div>
<input name="__VIEWSTATE" id="__VIEWSTATE" value="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" type="hidden">
</div>

<div>

	<input name="__EVENTVALIDATION" id="__EVENTVALIDATION" value="/wEWCAK2gf+KBALAlMXDBwLBlMXDBwLClMXDBwLDlMXDBwLElMXDBwLP+++tCwK5upDkC/CRLa9/p/fzdYmhE3CqavfjfWrI" type="hidden">
</div>

							
							<div id="contentdiv" class="text" itemprop="articleBody">
							



<ul class="download">
<li><a href="http://www.codeproject.com/KB/library/NeuralNetRecognition/Demo-Mnist.zip">Download the Neural Network demo project - 203 Kb</a> (includes a release-build executable that you can run without the need to compile) 
</li><li><a href="http://www.codeproject.com/KB/library/NeuralNetRecognition/simpleneutronweightfile.zip">Download a sample neuron weight file - 2,785 Kb</a> (achieves the 99.26% accuracy mentioned above) 
</li><li><a href="http://yann.lecun.com/exdb/mnist/index.html" target="_blank">Download the MNIST database - 11,594 Kb total for all four files</a>&nbsp;(external link to four files which are all required for this project) </li></ul>
<p><img alt="Graphical view of the neural network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Screenshot-GraphicalView.gif" border="0" height="372" width="600"></p><a name="topmost">
<h2>Contents</h2>
</a><ul type="disc"><a name="topmost">
</a><li><a name="topmost"></a><a href="#Introduction">Introduction</a> 
</li><li><a href="#Theory">Some Neural Network Theory</a> 
<ul type="disc">
<li><a href="#ForwardPropagation">Forward Propagation</a> 
</li><li><a href="#ActivationFunction">The Activation Function (or, "Sigmoid" or "Squashing" Function)</a> 
</li><li><a href="#Backpropagation">Backpropagation</a> 
</li><li><a href="#SecondOrder">Second Order Methods</a> </li></ul>
</li><li><a href="#ConvolutionalStructure">Structure of the Convolutional Neural Network</a> 
<ul type="disc">
<li><a href="#Illustration">Illustration and General Description</a> 
</li><li><a href="#CodeToBuild">Code For Building the Neural Network</a> </li></ul>
</li><li><a href="#AboutMNist">MNIST Database of Handwritten Digits</a> 
</li><li><a href="#Architecture">Overall Architecture of the Test/Demo Program</a> 
<ul type="disc">
<li><a href="#Using">Using the Demo Program</a> 
</li><li><a href="#GraphicalView">Graphical View of the Neural Network</a> 
</li><li><a href="#TrainingView">Training View and Control Over the Neural Network</a> 
</li><li><a href="#TestingView">Testing View of the Neural Network</a> </li></ul>
</li><li><a href="#Training">Training the Neural Network</a> 
</li><li><a href="#Tricks">Tricks That Make Training Faster</a> 
<ul type="disc">
<li><a href="#Hessian">Second Order Backpropagation Using Pseudo-Hessian</a> 
</li><li><a href="#SimultaneousBackprop">Simultaneous Backpropagation and Forward Propagation</a> 
</li><li><a href="#SkipBackprop">Skip Backpropagation for Small Errors</a> </li></ul>
</li><li><a href="#Experiences">Experiences in Training the Neural Network</a> 
</li><li><a href="#Results">Results</a> 
</li><li><a href="#Bibliography">Bibliography</a> 
</li><li><a href="#Version">License and Version Information</a> </li></ul><a name="Introduction">
<h2>Introduction</h2></a>
<p>This article chronicles the development of an artificial neural 
network designed to recognize handwritten digits. Although some theory 
of neural networks is given here, it would be better if you already 
understood some neural network concepts, like neurons, layers, weights, 
and backpropagation.</p>
<p>The neural network described here is <i>not</i> a general-purpose 
neural network, and it's not some kind of a neural network workbench. 
Rather, we will focus on one very specific neural network (a five-layer 
convolutional neural network) built for one very specific purpose (to 
recognize handwritten digits).</p>
<p>The idea of using neural networks for the purpose of recognizing 
handwritten digits is not a new one. The inspiration for the 
architecture described here comes from articles written by two separate 
authors. The first is Dr. Yann LeCun, who was an independent discoverer 
of the basic backpropagation algorithm. Dr. LeCun hosts <a href="http://yann.lecun.com/" target="_blank">an excellent site on his research into neural networks</a>. In particular, you should view his <a href="http://yann.lecun.com/exdb/lenet/index.html" target="_newwin">"Learning and Visual Perception"</a>
 section, which uses animated GIFs to show results of his research. The 
MNIST database (which provides the database of handwritten digits) was 
developed by him. I used two of his publications as primary source 
materials for much of my work, and I highly recommend reading his other 
publications too (they're posted at his site). Unlike many other 
publications on neural networks, Dr. LeCun's publications are not 
inordinately theoretical and math-intensive; rather, they are extremely 
readable, and provide practical insights and explanations. His articles 
and publications can be found <a href="http://yann.lecun.com/exdb/publis/index.html" target="_newwin">here</a>. Here are the two publications that I relied on:</p>
<ul>
<li>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" target="_newwin">"Gradient-Based Learning Applied to Document Recognition,"&nbsp;</a>Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998. [46 pages] 
</li><li>Y. LeCun, L. Bottou, G. Orr, and K. Muller, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_newwin">"Efficient BackProp,"&nbsp;</a>in Neural Networks: Tricks of the trade, (G. Orr and Muller K., eds.), 1998. [44 pages] </li></ul>
<p>The second author is Dr. Patrice Simard, a former collaborator with 
Dr. LeCun when they both worked at AT&amp;T Laboratories. Dr. Simard is 
now a researcher at Microsoft's <a href="http://www.research.microsoft.com/dpu/" target="_newwin">"Document Processing and Understanding"</a> group. His articles and publications can be found <a href="http://research.microsoft.com/%7Epatrice/publi.html" target="_newwin">here</a>, and the publication that I relied on is:</p>
<ul>
<li>Patrice Y. Simard, Dave Steinkraus, John Platt, <a href="http://research.microsoft.com/%7Epatrice/PDF/fugu9.pdf" target="_newwin">"Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis,"&nbsp;<img alt="External Link" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ExternalLink.html" border="0" height="14" width="14"></a> International Conference on Document Analysis and Recognition (ICDAR), IEEE Computer Society, Los Alamitos, pp. 958-962, 2003. </li></ul>
<p>One of my goals here was to reproduce the accuracy achieved by Dr. 
LeCun, who was able to train his neural network to achieve 99.18% 
accuracy (i.e., an error rate of only 0.82%). This error rate served as a
 type of "benchmark", guiding my work.</p>
<p>As a final introductory note, I'm not overly proud of the source 
code, which is most definitely an engineering work-in-progress. I 
started out with good intentions, to make source code that was flexible 
and easy to understand and to change. As things progressed, the code 
started to turn ugly. I began to write code simply to get the job done, 
sometimes at the expense of clean code and comprehensibility. To add to 
the mix, I was also experimenting with different ideas, some of which 
worked and some of which did not. As I removed the failed ideas, I did 
not always back out all the changes and there are therefore some 
dangling stubs and dead ends. I contemplated the possibility of not 
releasing the code. But that was one of my criticisms of the articles I 
read: none of them included code. So, with trepidation and the 
recognition that the code is easy to criticize and could really use a 
re-write, here it is.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Theory"></a>
<h2>Some Neural Network Theory</h2>
<p>This is not a neural network tutorial, but to understand the code and
 the names of the variables used in it, it helps to see some neural 
networks basics. </p>
<p>The following discussion is not completely general. It considers only
 feed-forward neural networks, that is, neural networks composed of 
multiple layers, in which each layer of neurons feeds only the very next
 layer of neurons, and receives input only from the immediately 
preceding layer of neurons. In other words, the neurons don't skip 
layers.</p>
<p>Consider a neural network that is composed of multiple layers, with 
multiple neurons in each layer. Focus on one neuron in layer <i>n</i>, namely the <i>i-th</i>
 neuron. This neuron gets its inputs from the outputs of neurons in the 
previous layer, plus a bias whose valued is one ("1"). I use the 
variable "<i>x</i>" to refer to outputs of neurons. The <i>i-th</i> 
neuron applies a weight to each of its inputs, and then adds the 
weighted inputs together so as to obtain something called the 
"activation value". I use the variable "<i>y</i>" to refer to activation values. The <i>i-th</i> neuron then calculates its output value "<i>x</i>" by applying an "activation function" to the activation value. I use the letter "<i>F()</i>"
 to refer to the activation function. The activation function is 
sometimes referred to as a "Sigmoid" function, a "Squashing" function 
and other names, since its primary purpose is to limit the output of the
 neuron to some reasonable range like a range of -1 to +1, and thereby 
inject some degree of non-linearity into the network. Here's a diagram 
of a small part of the neural network; remember to focus on the <i>i-th</i> neuron in level <i>n</i>:</p>
<p><img alt="General diagram of a neuron in a neural network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/NeuralNetDiagram.gif" border="0" height="543" width="573"> </p>
<p>This is what each variable means:</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Output of the i-th neuron in layer n" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/xNI.gif" border="0" height="25" width="21"></p></td>
<td>is the output of the <i>i</i>-th neuron in layer <i>n</i></td></tr>
<tr>
<td>
<p><img alt="Output of the j-th neuron in layer n-1" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/xNm1J.gif" border="0" height="25" width="32"></p></td>
<td>is the output of the <i>j</i>-th neuron in layer <i>n-1</i></td></tr>
<tr>
<td>
<p><img alt="Output of the k-th neuron in layer n-1" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/xNm1K.gif" border="0" height="25" width="30"></p></td>
<td>is the output of the <i>k</i>-th neuron in layer <i>n-1</i></td></tr>
<tr>
<td valign="top">
<p><img alt="Weight that the i-th neuron in layer n applies to the output of the j-th neuron from layer n-1" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/wNIJ.gif" border="0" height="25" width="27"></p></td>
<td>is the weight that the <i>i</i>-th neuron in layer <i>n</i> applies to the output of the <i>j</i>-th neuron from layer <i>n-1</i> (i.e., the previous layer). In other words, it's the weight from the output of the <i>j</i>-th neuron in the previous layer to the <i>i</i>-th neuron in the current (<i>n</i>-th) layer.</td></tr>
<tr>
<td>
<p><img alt="Weight that the i-th neuron in layer n applies to the output of the k-th neuron in layer n-1" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/wNIK.gif" border="0" height="25" width="27"></p></td>
<td>is the weight that the <i>i</i>-th neuron in layer <i>n</i> applies to the output of the <i>k</i>-th neuron in layer <i>n-1</i></td></tr></tbody></table>
<table>
<tbody>
<tr>
<td>
<p><img alt="General feed-forward equation" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ForwardPropagationEquation.gif" border="0" height="53" width="227"></p></td>
<td>is the general feed-forward equation, where <i>F()</i> is the activation function. We will discuss the activation function in more detail in a moment.</td></tr></tbody></table>
<p>How does this translate into code and C++ classes? The way I saw it, 
the above diagram suggested that a neural network is composed of objects
 of four different classes: layers, neurons in the layers, connections 
from neurons in one layer to those in another layer, and weights that 
are applied to connections. Those four classes are reflected in the 
code, together with a fifth class -- the neural network itself -- which 
acts as a container for all other objects and which serves as the main 
interface with the outside world. Here's a simplified view of the 
classes. Note that the code makes heavy use of <code>std::vector</code>, particularly <code>std::vector&lt; <span class="cpp-keyword">double</span> &gt;</code>:</p><div style="display:block" width="100%" id="premain0" class="pre-action-link"><img preid="0" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg0" height="9" width="9"><span preid="0" style="cursor: pointer; margin-bottom: 0px;" id="precollapse0"> Collapse</span><span> | </span><a preid="0" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre0"><span class="code-comment">//</span><span class="code-comment"> simplified view: some members have been omitted,
</span><span class="code-comment">//</span><span class="code-comment"> and some signatures have been altered
</span>
<span class="code-comment">//</span><span class="code-comment"> helpful typedef's
</span>
typedef std::vector&lt; NNLayer* &gt;  VectorLayers;
typedef std::vector&lt; NNWeight* &gt;  VectorWeights;
typedef std::vector&lt; NNNeuron* &gt;  VectorNeurons;
typedef std::vector&lt; NNConnection &gt; VectorConnections;


<span class="code-comment">//</span><span class="code-comment"> Neural Network class
</span>
<span class="code-keyword">class</span> NeuralNetwork  
{
<span class="code-keyword">public</span>:
    NeuralNetwork();
    <span class="code-keyword">virtual</span> ~NeuralNetwork();
    
    <span class="code-keyword">void</span> Calculate( double* inputVector, UINT iCount, 
        double* outputVector = NULL, UINT oCount = <span class="code-digit">0</span> );

    <span class="code-keyword">void</span> Backpropagate( <span class="code-keyword">double</span> *actualOutput, 
         <span class="code-keyword">double</span> *desiredOutput, UINT count );

    VectorLayers m_Layers;
};


<span class="code-comment">//</span><span class="code-comment"> Layer class
</span>
<span class="code-keyword">class</span> NNLayer
{
<span class="code-keyword">public</span>:
    NNLayer( LPCTSTR str, NNLayer* pPrev = NULL );
    <span class="code-keyword">virtual</span> ~NNLayer();
    
    <span class="code-keyword">void</span> Calculate();
    
    <span class="code-keyword">void</span> Backpropagate( std::vector&lt; <span class="code-keyword">double</span> &gt;&amp; dErr_wrt_dXn <span class="code-comment">/*</span><span class="code-comment"> in */</span>, 
        std::vector&lt; <span class="code-keyword">double</span> &gt;&amp; dErr_wrt_dXnm1 <span class="code-comment">/*</span><span class="code-comment"> out */</span>, 
        <span class="code-keyword">double</span> etaLearningRate );

    NNLayer* m_pPrevLayer;
    VectorNeurons m_Neurons;
    VectorWeights m_Weights;
};


<span class="code-comment">//</span><span class="code-comment"> Neuron class
</span>
<span class="code-keyword">class</span> NNNeuron
{
<span class="code-keyword">public</span>:
    NNNeuron( LPCTSTR str );
    <span class="code-keyword">virtual</span> ~NNNeuron();

    <span class="code-keyword">void</span> AddConnection( UINT iNeuron, UINT iWeight );
    <span class="code-keyword">void</span> AddConnection( NNConnection <span class="code-keyword">const</span> &amp; conn );

    <span class="code-keyword">double</span> output;

    VectorConnections m_Connections;
};


<span class="code-comment">//</span><span class="code-comment"> Connection class
</span>
<span class="code-keyword">class</span> NNConnection
{
<span class="code-keyword">public</span>: 
    NNConnection(UINT neuron = ULONG_MAX, UINT weight = ULONG_MAX);
    <span class="code-keyword">virtual</span> ~NNConnection();

    UINT NeuronIndex;
    UINT WeightIndex;
};


<span class="code-comment">//</span><span class="code-comment"> Weight class
</span>
<span class="code-keyword">class</span> NNWeight
{
<span class="code-keyword">public</span>:
    NNWeight( LPCTSTR str, <span class="code-keyword">double</span> val = <span class="code-digit">0</span>.<span class="code-digit">0</span> );
    <span class="code-keyword">virtual</span> ~NNWeight();

    <span class="code-keyword">double</span> value;
};
</pre>
<p>As you can see from the above, class <code>NeuralNetwork</code> stores a vector of pointers to layers in the neural network, which are represented by class <code>NNLayer</code>. There is no special function to add a layer (there probably should be one); simply use the <code>std::vector::push_back()</code> function. The <code>NeuralNetwork</code>
 class also provides the two primary interfaces with the outside world, 
namely, a function to forward propagate the neural network (the <code>Calculate()</code> function) and a function to <code>Backpropagate()</code> the neural network so as to train it.</p>
<p>Each <code>NNLayer</code> stores a pointer to the previous layer, so 
that it knows where to look for its input values. In addition, it stores
 a vector of pointers to the neurons in the layer, represented by class <code>NNNeuron</code>, and a vector of pointers to weights, represented by class <code>NNWeight</code>. Similar to the <code>NeuralNetwork</code> class, the pointers to the neurons and to the weights are added using the <code>std::vector::push_back()</code> function. Finally, the <code>NNLayer</code> class includes functions to <code>Calculate()</code> the output values of neurons in the layer, and to <code>Backpropagate()</code> them; in fact, the corresponding function in the <code>NeuralNetwork</code> class simply iterate through all layers in the network and call these functions.</p>
<p>Each <code>NNNeuron</code> stores a vector of connections that tell the neurons where to get its inputs. Connections are added using the <code>NNNeuron::AddConnection()</code> function, which takes an index to a neuron and an index to a weight, constructs a <code>NNConnection</code> object, and <code>push_back()</code>'s the new connection onto the vector of connections. Each neuron also stores its own output value, even though it's the <code>NNLayer</code> class that is responsible for calculating the actual value of the output and storing it there. The <code>NNConnection</code> and <code>NNWeight</code> classes respectively store obviously-labeled information.</p>
<p>One legitimate question about the class structure is, why are there 
separate classes for the weights and the connections? According to the 
diagram above, each connection has a weight, so why not put them in the 
same class? The answer lies in the fact that weights are often shared 
between connections. In fact, the convolutional neural network of this 
program specifically shares weights amongst its connections. So, for 
example, even though there might be several hundred neurons in a layer, 
there might only be a few dozen weights due to sharing. By making the <code>NNWeight</code> class separate from the <code>NNConnection</code> class, this sharing is more readily accomplished.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="ForwardPropagation"></a>
<h3>Forward Propagation</h3>
<p>Forward propagation is the process whereby each of all of the neurons
 calculates its output value, based on inputs provided by the output 
values of the neurons that feed it.</p>
<p>In the code, the process is initiated by calling <code>NeuralNetwork::Calculate()</code>. <code>NeuralNetwork::Calculate()</code> directly sets the values of neurons in the input layer, and then iterates through the remaining layers, calling each layer's <code>NNLayer::Calculate()</code>
 function. This results in a forward propagation that's completely 
sequential, starting from neurons in the input layer and progressing 
through to the neurons in the output layer. A sequential calculation is 
not the only way to forward propagate, but it's the most 
straightforward. Here's simplified code, which takes a pointer to a 
C-style array of <code><span class="cpp-keyword">double</span></code>s representing the input to the neural network, and stores the output of the neural network to another C-style array of <code><span class="cpp-keyword">double</span></code>s:</p><div style="display:block" width="100%" id="premain1" class="pre-action-link"><img preid="1" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg1" height="9" width="9"><span preid="1" style="cursor: pointer; margin-bottom: 0px;" id="precollapse1"> Collapse</span><span> | </span><a preid="1" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre1"><span class="code-comment">//</span><span class="code-comment"> simplified code
</span>
<span class="code-keyword">void</span> NeuralNetwork::Calculate(double* inputVector, UINT iCount, 
               double* outputVector <span class="code-comment">/*</span><span class="code-comment"> =NULL */</span>, 
               UINT oCount <span class="code-comment">/*</span><span class="code-comment"> =0 */</span>)
                              
{
    VectorLayers::iterator lit = m_Layers.begin();
    VectorNeurons::iterator nit;
    
    <span class="code-comment">//</span><span class="code-comment"> first layer is input layer: directly
</span>    <span class="code-comment">//</span><span class="code-comment"> set outputs of all of its neurons
</span>    <span class="code-comment">//</span><span class="code-comment"> to the given input vector
</span>    
    <span class="code-keyword">if</span> ( lit &lt; m_Layers.end() )  
    {
        nit = (*lit)-&gt;m_Neurons.begin();
        <span class="code-keyword">int</span> count = <span class="code-digit">0</span>;
        
        ASSERT( iCount == (*lit)-&gt;m_Neurons.size() );
        <span class="code-comment">//</span><span class="code-comment"> there should be exactly one neuron per input
</span>        
        <span class="code-keyword">while</span>( ( nit &lt; (*lit)-&gt;m_Neurons.end() ) &amp;&amp; ( count &lt; iCount ) )
        {
            (*nit)-&gt;output = inputVector[ count ];
            nit++;
            count++;
        }
    }
    
    <span class="code-comment">//</span><span class="code-comment"> iterate through remaining layers,
</span>    <span class="code-comment">//</span><span class="code-comment"> calling their Calculate() functions
</span>    
    <span class="code-keyword">for</span>( lit++; lit&lt;m_Layers.end(); lit++ )
    {
        (*lit)-&gt;Calculate();
    }
    
    <span class="code-comment">//</span><span class="code-comment"> load up output vector with results
</span>    
    <span class="code-keyword">if</span> ( outputVector != NULL )
    {
        lit = m_Layers.end();
        lit--;
        
        nit = (*lit)-&gt;m_Neurons.begin();
        
        <span class="code-keyword">for</span> ( <span class="code-keyword">int</span> ii=0; ii&lt;oCount; ++ii )
        {
            outputVector[ ii ] = (*nit)-&gt;output;
            nit++;
        }
    }
}
</pre>
<p>Inside the layer's <code>Calculate()</code> function, the layer 
iterates through all neurons in the layer, and for each neuron the 
output is calculated according to the feed-forward formula given above, 
namely</p>
<p><img alt="General feed-forward equation" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ForwardPropagationEquation.gif" border="0" height="53" width="227"></p>
<p>This formula is applied by iterating through all connections for the 
neuron, and for each connection, obtaining the corresponding weight and 
the corresponding output value from a neuron in the previous layer:</p><div style="display:block" width="100%" id="premain2" class="pre-action-link"><img preid="2" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg2" height="9" width="9"><span preid="2" style="cursor: pointer; margin-bottom: 0px;" id="precollapse2"> Collapse</span><span> | </span><a preid="2" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre2"><span class="code-comment">//</span><span class="code-comment"> simplified code
</span>
<span class="code-keyword">void</span> NNLayer::Calculate()
{
    ASSERT( m_pPrevLayer != NULL );
    
    VectorNeurons::iterator nit;
    VectorConnections::iterator cit;
    
    <span class="code-keyword">double</span> dSum;
    
    <span class="code-keyword">for</span>( nit=m_Neurons.begin(); nit&lt;m_Neurons.end(); nit++ )
    {
        NNNeuron&amp; n = *(*nit);  <span class="code-comment">//</span><span class="code-comment"> to ease the terminology
</span>        
        cit = n.m_Connections.begin();
        
        ASSERT( (*cit).WeightIndex &lt; m_Weights.size() );
        
        <span class="code-comment">//</span><span class="code-comment"> weight of the first connection is the bias;
</span>        <span class="code-comment">//</span><span class="code-comment"> its neuron-index is ignored
</span>
        dSum = m_Weights[ (*cit).WeightIndex ]-&gt;value;  
        
        <span class="code-keyword">for</span> ( cit++ ; cit&lt;n.m_Connections.end(); cit++ )
        {
            ASSERT( (*cit).WeightIndex &lt; m_Weights.size() );
            ASSERT( (*cit).NeuronIndex &lt; 
                     m_pPrevLayer-&gt;m_Neurons.size() );
            
            dSum += ( m_Weights[ (*cit).WeightIndex ]-&gt;value ) * 
                ( m_pPrevLayer-&gt;m_Neurons[ 
                   (*cit).NeuronIndex ]-&gt;output );
        }
        
        n.output = SIGMOID( dSum );
        
    }
    
}</pre>
<p>In this code, <code>SIGMOID</code> is <code><span class="cpp-preprocessor">#define</span></code>d to the activation function, which is described in the next section. </p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="ActivationFunction"></a>
<h3>The Activation Function (or, "Sigmoid" or "Squashing" Function)</h3>
<p>Selection of a good activation function is an important part of the 
design of a neural network. Generally speaking, the activation function 
should be symmetric, and the neural network should be trained to a value
 that is lower than the limits of the function.</p>
<p>One function that should <i><b>never</b></i> be used as the activation function is the classical sigmoid function (or "logistic" function), defined as <img alt="Logisitc function" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/LogisticFunction.gif" align="middle" border="0" height="42" width="110">.
 It should never be used since it is not symmetric: its value approaches
 +1 for increasing x, but for decreasing x its value approaches zero 
(i.e., it does not approach -1 which it should for symmetry). The reason
 the logistic function is even mentioned here is that there are many 
articles on the web that recommend its use in neural networks, for 
example, <a href="http://en.wikipedia.org/wiki/Sigmoid_function" target="_newwin">Sigmoid function</a> in the Wikipedia. In my view, this is a poor recommendation and should be avoided.</p>
<p>One good selection for the activation function is the hyperbolic tangent, or <img alt="Hyperbolic tangent" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/HyperbolicTangent.gif" border="0" height="22" width="117">.
 This function is a good choice because it's completely symmetric, as 
shown in the following graph. If used, then do not train the neural 
network to ±1.0. Instead, choose an intermediate value, like ±0.8.</p>
<p><img alt="Graph of hyperbolic tangent" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/HyperbolicTangentGraph.gif" border="0" height="220" width="327"></p>
<p>Another reason why hyperbolic tangent is a good choice is that it's 
easy to obtain its derivative. Yes, sorry, but a bit of calculus is 
needed in neural networks. Not only is it easy to obtain its derivative,
 but also the value of derivative can be expressed in terms of the 
output value (i.e., as opposed to the input value). More specifically, 
given that:</p>
<p><img alt="Basic tanh function" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/DerivativeEquation1.gif" align="middle" border="0" height="50" width="219">&nbsp;,
 where (using the notation established above) y is the input to the 
function (corresponding to the activation value of a neuron) and x is 
the output of the neuron.</p>
<p>Then <img alt="Derivative of tanh function" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/DerivativeEquation2.gif" align="middle" border="0" height="55" width="324"></p>
<p>Which simplifies to <img alt="Simplified derivative" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/DerivativeEquation3.gif" align="middle" border="0" height="47" width="140"> </p>
<p>Or, since <img alt="Given values" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/DerivativeEquation4.gif" align="middle" border="0" height="21" width="86">, the result is <img alt="Result" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/DerivativeEquation5.gif" align="middle" border="0" height="47" width="90">. This result means that we can calculate the value of the derivative of <i>F()</i>
 given only the output of the function, without any knowledge of the 
input. In this article, we will refer to the derivative of the 
activation function as <i>G(x)</i>.</p>
<p>The activation function used in the code is a scaled version of the 
hyperbolic tangent. It was chosen based on a recommendation in one of 
Dr. LeCun's articles. Scaling causes the function to vary between 
±1.7159, and permits us to train the network to values of ±1.0.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Backpropagation"></a>
<h3>Backpropagation</h3>
<p>Backpropagation is an iterative process that starts with the last 
layer and moves backwards through the layers until the first layer is 
reached. Assume that for each layer, we know the error in the output of 
the layer. If we know the error of the output, then it is not hard to 
calculate changes for the weights, so as to reduce that error. The 
problem is that we can only observe the error in the output of the very 
last layer.</p>
<p>Backpropagation gives us a way to determine the error in the output 
of a prior layer given the output of a current layer. The process is 
therefore iterative: start at the last layer and calculate the change in
 the weights for the last layer. Then calculate the error in the output 
of the prior layer. Repeat.</p>
<p>The backpropagation equations are given below. My purpose in showing 
you the equations is so that you can find them in the code, and 
understand them. For example, the first equation shows how to calculate 
the partial derivative of the error <i>E<sup>P</sup></i> with respect to the activation value <i>y<sup>i</sup></i> at the <i>n-th</i> layer. In the code, you will see a variable named <code>dErr_wrt_dYn[ ii ]</code>.</p>
<p>Start the process off by computing the partial derivative of the 
error due to a single input image pattern with respect to the outputs of
 the neurons on the last layer. The error due to a single pattern is 
calculated as follows:</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Equation (1): Error due to a single pattern" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/eqtn1-ErrNP.gif" border="0" height="46" width="157"></p></td>
<td>(equation 1)</td></tr></tbody></table>
<p>where:</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Error due to a single pattern P at the last layer n" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ErrNP.gif" border="0" height="25" width="25"></p></td>
<td>is the error due to a single pattern <i>P</i> at the last layer <i>n</i>;</td></tr>
<tr>
<td>
<p><img alt="Target output at the last layer (i.e., the desired output at the last layer)" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/TNI.gif" border="0" height="25" width="21"></p></td>
<td>is the target output at the last layer (i.e., the desired output at the last layer); and</td></tr>
<tr>
<td>
<p><img alt="Actual value of the output at the last layer" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/xNI.gif" border="0" height="25" width="21"></p></td>
<td>is the actual value of the output at the last layer.</td></tr></tbody></table>
<p>Given equation (1), then taking the partial derivative yields:</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Equation (2): Partial derivative of the output error for one pattern with respect to the neuron output values" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/eqtn2-dErrNP-wrt-dxNI.gif" border="0" height="54" width="97"></p></td>
<td>(equation 2)</td></tr></tbody></table>
<p>Equation (2) gives us a starting value for the backpropagation 
process. We use the numeric values for the quantities on the right side 
of equation (2) in order to calculate numeric values for the derivative.
 Using the numeric values of the derivative, we calculate the numeric 
values for the changes in the weights, by applying the following two 
equations (3) and then (4):</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Equation (3): Partial derivative of the output error for one pattern with respect to the activation value of each neuron" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/eqtn3-dErrNP-wrt-dyNI.gif" border="0" height="57" width="148"></p></td>
<td>(equation 3)</td></tr></tbody></table>
<table>
<tbody>
<tr>
<td>where</td>
<td>
<p><img alt="Derivative of the activation function" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/G-of-xNI.gif" border="0" height="25" width="48"></p></td>
<td>is the derivative of the activation function.</td></tr></tbody></table>
<table>
<tbody>
<tr>
<td><img alt="Equation (4): Partial derivative of the output error for one pattern with respect to each weight feeding the neuron" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/eqtn4-dErrNP-wrt-dWNij.gif" border="0" height="57" width="133"></td>
<td>(equation 4)</td></tr></tbody></table>
<p>Then, using equation (2) again and also equation (3), we calculate 
the error for the previous layer, using the following equation (5):</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Equation (5): Partial derivative of the error for the previous layer" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/eqtn5-dErrNm1P-wrt-dxNm1K.gif" border="0" height="55" width="161"></p></td>
<td>(equation 5)</td></tr></tbody></table>
<p>The values we obtain from equation (5) are used as starting values for the calculations on the immediately preceding layer. <i><b>This is the single most important point in understanding backpropagation.</b></i>
 In other words, we take the numeric values obtained from equation (5), 
and use them in a repetition of equations (3), (4) and (5) for the 
immediately preceding layer.</p>
<p>Meanwhile, the values from equation (4) tell us how much to change 
the weights in the current layer n, which was the whole purpose of this 
gigantic exercise. In particular, we update the value of each weight 
according to the formula:</p>
<table>
<tbody>
<tr>
<td>
<p><img alt="Equation (6): Updating the weights" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/eqtn6-UpdateWeights.gif" border="0" height="55" width="245"></p></td>
<td>(equation 6)</td></tr></tbody></table>
<p>where <i>eta</i> is the "learning rate", typically a small number like 0.0005 that is gradually decreased during training.</p>
<p>In the code, these equations are implemented by calling <code>NeuralNetwork::Backpropagate()</code>. The input to the <code>NeuralNetwork::Backpropagate()</code> function is the actual output of the neural network and the desired output. Using these two inputs, the <code>NeuralNetwork::Backpropagate()</code>
 function calculates the value of equation (2). It then iterates through
 all layers in the network, starting from the last layer and proceeding 
backwards toward the first layer. For each layer, the layer's <code>NNLayer::Backpropagate()</code> function is called. The input to <code>NNLayer::Backpropagate()</code> is the derivative, and the output is equation (5). These derivatives are all stored in a <code>vector</code> of a <code>vector</code> of <code>doubles</code> named <code>differentials</code>. The output from one layer is then fed as the input to the next preceding layer:</p><div style="display:block" width="100%" id="premain3" class="pre-action-link"><img preid="3" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg3" height="9" width="9"><span preid="3" style="cursor: pointer; margin-bottom: 0px;" id="precollapse3"> Collapse</span><span> | </span><a preid="3" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre3"><span class="code-comment">//</span><span class="code-comment"> simplified code
</span><span class="code-keyword">void</span> NeuralNetwork::Backpropagate(<span class="code-keyword">double</span> *actualOutput, 
     <span class="code-keyword">double</span> *desiredOutput, UINT count)
{
    <span class="code-comment">//</span><span class="code-comment"> Backpropagates through the neural net
</span>    <span class="code-comment">//</span><span class="code-comment"> Proceed from the last layer to the first, iteratively
</span>    <span class="code-comment">//</span><span class="code-comment"> We calculate the last layer separately, and first,
</span>    <span class="code-comment">//</span><span class="code-comment"> since it provides the needed derviative
</span>    <span class="code-comment">//</span><span class="code-comment"> (i.e., dErr_wrt_dXnm1) for the previous layers
</span>    
    <span class="code-comment">//</span><span class="code-comment"> nomenclature:
</span>    <span class="code-comment">//</span><span class="code-comment">
</span>    <span class="code-comment">//</span><span class="code-comment"> Err is output error of the entire neural net
</span>    <span class="code-comment">//</span><span class="code-comment"> Xn is the output vector on the n-th layer
</span>    <span class="code-comment">//</span><span class="code-comment"> Xnm1 is the output vector of the previous layer
</span>    <span class="code-comment">//</span><span class="code-comment"> Wn is the vector of weights of the n-th layer
</span>    <span class="code-comment">//</span><span class="code-comment"> Yn is the activation value of the n-th layer,
</span>    <span class="code-comment">//</span><span class="code-comment"> i.e., the weighted sum of inputs BEFORE 
</span>    <span class="code-comment">//</span><span class="code-comment">    the squashing function is applied
</span>    <span class="code-comment">//</span><span class="code-comment"> F is the squashing function: Xn = F(Yn)
</span>    <span class="code-comment">//</span><span class="code-comment"> F' is the derivative of the squashing function
</span>    <span class="code-comment">//</span><span class="code-comment">   Conveniently, for F = tanh,
</span>    <span class="code-comment">//</span><span class="code-comment">   then F'(Yn) = 1 - Xn^2, i.e., the derivative can be 
</span>    <span class="code-comment">//</span><span class="code-comment">   calculated from the output, without knowledge of the input
</span>    
    
    VectorLayers::iterator lit = m_Layers.end() - <span class="code-digit">1</span>;
    
    std::vector&lt; <span class="code-keyword">double</span> &gt; dErr_wrt_dXlast( (*lit)-&gt;m_Neurons.size() );
    std::vector&lt; std::vector&lt; <span class="code-keyword">double</span> &gt; &gt; differentials;
    
    <span class="code-keyword">int</span> iSize = m_Layers.size();
    
    differentials.resize( iSize );
    
    <span class="code-keyword">int</span> ii;
    
    <span class="code-comment">//</span><span class="code-comment"> start the process by calculating dErr_wrt_dXn for the last layer.
</span>    <span class="code-comment">//</span><span class="code-comment"> for the standard MSE Err function
</span>    <span class="code-comment">//</span><span class="code-comment"> (i.e., 0.5*sumof( (actual-target)^2 ), this differential is simply
</span>    <span class="code-comment">//</span><span class="code-comment"> the difference between the target and the actual
</span>    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;(*lit)-&gt;m_Neurons.size(); ++ii )
    {
        dErr_wrt_dXlast[ ii ] = 
            actualOutput[ ii ] - desiredOutput[ ii ];
    }
    
    
    <span class="code-comment">//</span><span class="code-comment"> store Xlast and reserve memory for
</span>    <span class="code-comment">//</span><span class="code-comment"> the remaining vectors stored in differentials
</span>    
    differentials[ iSize-1 ] = dErr_wrt_dXlast;  <span class="code-comment">//</span><span class="code-comment"> last one
</span>    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;iSize-1; ++ii )
    {
        differentials[ ii ].resize( 
             m_Layers[ii]-&gt;m_Neurons.size(), <span class="code-digit">0</span>.<span class="code-digit">0</span> );
    }
    
    <span class="code-comment">//</span><span class="code-comment"> now iterate through all layers including
</span>    <span class="code-comment">//</span><span class="code-comment"> the last but excluding the first, and ask each of
</span>    <span class="code-comment">//</span><span class="code-comment"> them to backpropagate error and adjust
</span>    <span class="code-comment">//</span><span class="code-comment"> their weights, and to return the differential
</span>    <span class="code-comment">//</span><span class="code-comment"> dErr_wrt_dXnm1 for use as the input value
</span>    <span class="code-comment">//</span><span class="code-comment"> of dErr_wrt_dXn for the next iterated layer
</span>    
    ii = iSize - <span class="code-digit">1</span>;
    <span class="code-keyword">for</span> ( lit; lit&gt;m_Layers.begin(); lit--)
    {
        (*lit)-&gt;Backpropagate( differentials[ ii ], 
              differentials[ ii - <span class="code-digit">1</span> ], m_etaLearningRate );
        --ii;
    }
    
    differentials.clear();
}
</pre>
<p>Inside the <code>NNLayer::Backpropagate()</code> function, the layer 
implements equations (3) through (5) in order to determine the 
derivative for use by the next preceding layer. It then implements 
equation (6) in order to update the weights in its layer. In the 
following code, the derivative of the activation function <i>G(x)</i> is <code><span class="cpp-preprocessor">#define</span></code>d as <code>DSIGMOID:</code></p><div style="display:block" width="100%" id="premain4" class="pre-action-link"><img preid="4" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg4" height="9" width="9"><span preid="4" style="cursor: pointer; margin-bottom: 0px;" id="precollapse4"> Collapse</span><span> | </span><a preid="4" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre4"><span class="code-comment">//</span><span class="code-comment"> simplified code
</span>
<span class="code-keyword">void</span> NNLayer::Backpropagate( std::vector&lt; <span class="code-keyword">double</span> &gt;&amp; dErr_wrt_dXn <span class="code-comment">/*</span><span class="code-comment"> in */</span>, 
                            std::vector&lt; <span class="code-keyword">double</span> &gt;&amp; dErr_wrt_dXnm1 <span class="code-comment">/*</span><span class="code-comment"> out */</span>, 
                            <span class="code-keyword">double</span> etaLearningRate )
{
    <span class="code-keyword">double</span> output;

    <span class="code-comment">//</span><span class="code-comment"> calculate equation (3): dErr_wrt_dYn = F'(Yn) * dErr_wrt_Xn
</span>    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;m_Neurons.size(); ++ii )
    {
        output = m_Neurons[ ii ]-&gt;output;
    
        dErr_wrt_dYn[ ii ] = DSIGMOID( output ) * dErr_wrt_dXn[ ii ];
    }
    
    <span class="code-comment">//</span><span class="code-comment"> calculate equation (4): dErr_wrt_Wn = Xnm1 * dErr_wrt_Yn
</span>    <span class="code-comment">//</span><span class="code-comment"> For each neuron in this layer, go through
</span>    <span class="code-comment">//</span><span class="code-comment"> the list of connections from the prior layer, and
</span>    <span class="code-comment">//</span><span class="code-comment"> update the differential for the corresponding weight
</span>    
    ii = <span class="code-digit">0</span>;
    <span class="code-keyword">for</span> ( nit=m_Neurons.begin(); nit&lt;m_Neurons.end(); nit++ )
    {
        NNNeuron&amp; n = *(*nit);  <span class="code-comment">//</span><span class="code-comment"> for simplifying the terminology
</span>        
        <span class="code-keyword">for</span> ( cit=n.m_Connections.begin(); cit&lt;n.m_Connections.end(); cit++ )
        {
            kk = (*cit).NeuronIndex;
            <span class="code-keyword">if</span> ( kk == ULONG_MAX )
            {
                output = <span class="code-digit">1</span>.<span class="code-digit">0</span>;  <span class="code-comment">//</span><span class="code-comment"> this is the bias weight
</span>            }
            <span class="code-keyword">else</span>
            {
                output = m_pPrevLayer-&gt;m_Neurons[ kk ]-&gt;output;
            }
            
            dErr_wrt_dWn[ (*cit).WeightIndex ] += dErr_wrt_dYn[ ii ] * output;
        }
        
        ii++;
    }
    
    
    <span class="code-comment">//</span><span class="code-comment"> calculate equation (5): dErr_wrt_Xnm1 = Wn * dErr_wrt_dYn,
</span>    <span class="code-comment">//</span><span class="code-comment"> which is needed as the input value of
</span>    <span class="code-comment">//</span><span class="code-comment"> dErr_wrt_Xn for backpropagation of the next (i.e., previous) layer
</span>    <span class="code-comment">//</span><span class="code-comment"> For each neuron in this layer
</span>    
    ii = <span class="code-digit">0</span>;
    <span class="code-keyword">for</span> ( nit=m_Neurons.begin(); nit&lt;m_Neurons.end(); nit++ )
    {
        NNNeuron&amp; n = *(*nit);  <span class="code-comment">//</span><span class="code-comment"> for simplifying the terminology
</span>        
        <span class="code-keyword">for</span> ( cit=n.m_Connections.begin(); 
              cit&lt;n.m_Connections.end(); cit++ )
        {
            kk=(*cit).NeuronIndex;
            <span class="code-keyword">if</span> ( kk != ULONG_MAX )
            {
                <span class="code-comment">//</span><span class="code-comment"> we exclude ULONG_MAX, which signifies
</span>                <span class="code-comment">//</span><span class="code-comment"> the phantom bias neuron with
</span>                <span class="code-comment">//</span><span class="code-comment"> constant output of "1",
</span>                <span class="code-comment">//</span><span class="code-comment"> since we cannot train the bias neuron
</span>                
                nIndex = kk;
                
                dErr_wrt_dXnm1[ nIndex ] += dErr_wrt_dYn[ ii ] * 
                       m_Weights[ (*cit).WeightIndex ]-&gt;value;
            }
            
        }
        
        ii++;  <span class="code-comment">//</span><span class="code-comment"> ii tracks the neuron iterator
</span>        
    }
    
    
    <span class="code-comment">//</span><span class="code-comment"> calculate equation (6): update the weights
</span>    <span class="code-comment">//</span><span class="code-comment"> in this layer using dErr_wrt_dW (from 
</span>    <span class="code-comment">//</span><span class="code-comment"> equation (4)    and the learning rate eta
</span>
    <span class="code-keyword">for</span> ( jj=0; jj&lt;m_Weights.size(); ++jj )
    {
        oldValue = m_Weights[ jj ]-&gt;value;
        newValue = oldValue.dd - etaLearningRate * dErr_wrt_dWn[ jj ];
        m_Weights[ jj ]-&gt;value = newValue;
    }
}</pre>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="SecondOrder">
<h3>Second Order Methods</h3></a>
<p>All second order techniques have one goal in mind: to increase the 
speed with which backpropagation converges to optimal weights. All 
second order techniques (at least in principle) accomplish this in the 
same fundamental way: by adjusting each weight differently, e.g., by 
applying a learning rate <i>eta</i> that differs for each individual weight.</p>
<p>In his <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_newwin">"Efficient BackProp,</a>
 article, Dr. LeCun proposes a second order technique that he calls the 
"stochastic diagonal Levenberg-Marquardt method". He compares the 
performance of this method with a "carefully tuned stochastic gradient 
algorithm", which is an algorithm that does not rely on second order 
techniques, but which does apply different learning rates to each 
individual weight. According to his comparisons, he concludes that "the 
additional cost <i>[of stochastic diagonal Levenberg-Marquardt]</i> over
 regular backpropagation is negligible and convergence is - as a rule of
 thumb - about three times faster than a carefully tuned stochastic 
gradient algorithm." (See page 35 of the article.)</p>
<p>It was clear to me that I needed a second order algorithm. 
Convergence without it was tediously slow. Dr. Simard, in his article 
titled <a href="http://research.microsoft.com/%7Epatrice/PDF/fugu9.pdf" target="_newwin">"Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis,"</a> indicated that he wanted to keep his algorithm as simple as possible and therefore did <i><b>not</b></i>
 use second order techniques. He also admitted that he required hundreds
 of epochs for convergence (my guess is that he required a few 
thousand).</p>
<p>With the MNIST database, each epoch requires 60,000 backpropagations,
 and on my computer each epoch took around 40 minutes. I did not have 
the patience (or confidence in the correctness of the code) to wait for 
thousands of epochs. It was also clear that, unlike Dr. LeCun, I did not
 have the skill to design "a carefully tuned stochastic gradient 
algorithm". So, in keeping with the advice that stochastic diagonal 
Levenberg-Marquardt would be around three times faster than that anyway,
 my neural network implements this second order technique.</p>
<p>I will not go into the math or the code for the stochastic diagonal 
Levenberg-Marquardt algorithm. It's actually not too dissimilar from 
standard backpropagation. Using this technique, I was able to achieve 
good convergence in around 20-25 epochs. In my mind this was terrific 
for two reasons. First, it increased my confidence that the network was 
performing correctly, since Dr. LeCun also reported convergence in 
around 20 epochs. Second, at 40 minutes per epoch, the network converged
 in around 14-16 hours, which is a palatable amount of time for an 
overnight run.</p>
<p>If you have the inclination to inspect the code on this point, the functions you want to focus on are named <code>CMNistDoc::CalculateHessian()</code> (which is in the document class - yes, the program is an MFC doc/view program), and <code>NeuralNetwork::BackpropagateSecondDervatives()</code>. In addition, you should note that the <code>NNWeight</code> class includes a <code><span class="cpp-keyword">double</span></code> member that was not mentioned in the simplified view above. This member is named "<code>diagHessian</code>", and it stores the curvature (in weight space) that is calculated by Dr. LeCun's algorithm. Basically, when <code>CMNistDoc::CalculateHessian()</code> is called, 500 MNIST patterns are selected at random. For each pattern, the <code>NeuralNetwork::BackpropagateSecondDervatives()</code> function calculates the Hessian for each weight caused by the pattern, and this number is accumulated in <code>diagHessian</code>. After the 500 patterns are run, the value in <code>diagHessian</code> is divided by 500, which results in a unique value of <code>diagHessian</code> for each and every weight. During actual backpropagation, the <code>diagHessian</code> value is used to amplify the current learning rate <i>eta</i>,
 such that in highly curved areas in weight space, the learning rate is 
slowed, whereas in flat areas in weight space, the learning rate is 
amplified.</p>
<p><a href="#topmost"><small>go back to top</small></a> <a name="ConvolutionalStructure"></a>
</p><h2>Structure of the Convolutional Neural Network</h2>
<p>As indicated above, the program does not implement a generalized 
neural network, and is not a neural network workbench. Rather, it is a 
very specific neural network, namely, a five-layer convolutional neural 
network. The input layer takes grayscale data of a 29x29 image of a 
handwritten digit, and the output layer is composed of ten neurons of 
which exactly one neuron has a value of +1 corresponding to the answer 
(hopefully) while all other nine neurons have an output of -1.</p>
<p>Convolutional neural networks are also known as "shared weight" 
neural networks. The idea is that a small kernel window is moved over 
neurons from a prior layer. In this network, I use a kernel sized to 5x5
 elements. Each element in the 5x5 kernel window has a weight 
independent of that of another element, so there are 25 weights (plus 
one additional weight for the bias term). This kernel is shared across 
all elements in the prior layer, hence the name "shared weight". A more 
detailed explanation follows.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Illustration"></a>
<h3>Illustration and General Description</h3>
<p>Here is an illustration of the neural network:</p>
<table>
<tbody>
<tr>
<td colspan="5" width="599">
<p><img alt="Illustration of the Neural Network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/IllustrationNeuralNet.gif" border="0" height="300" width="599"></p></td></tr>
<tr>
<td align="center" width="229">Input Layer<br>29x29</td>
<td align="center" width="140">Layer #1<br>6 Feature Maps<br>Each 13x13</td>
<td align="center" width="75">Layer #2<br>50 Feature Maps<br>Each 5x5</td>
<td align="center" width="75">Layer #3<br>Fully Connected<br>100 Neurons</td>
<td align="center" width="80">Layer #4<br>Fully Connected<br>10 Neurons</td></tr></tbody></table>
<p>The input layer (Layer #0) is the grayscale image of the handwritten 
character. The MNIST image database has images whose size is 28x28 
pixels each, but because of the considerations described by Dr. Simard 
in his article <a href="http://research.microsoft.com/%7Epatrice/PDF/fugu9.pdf" target="_newwin">"Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis,"</a> the image size is padded to 29x29 pixels. There are therefore 29x29 = 841 neurons in the input layer.</p>
<p>Layer #1 is a convolutional layer with six (6) feature maps. Each 
feature map is sized to 13x13 pixels/neurons. Each neuron in each 
feature map is a 5x5 convolutional kernel of the input layer, but every 
other pixel of the input layer is skipped (as described in Dr. Simard's 
article). As a consequence, there are 13 positions where the 5x5 kernel 
will fit in each row of the input layer (which is 29 neurons wide), and 
13 positions where the 5x5 kernel will fit in each column of the input 
layer (which is 29 neurons high). There are therefore 13x13x6 = 1014 
neurons in Layer #1, and (5x5+1)x6 = 156 weights. (The "+1" is for the 
bias.)</p>
<p>On the other hand, since each of the 1014 neurons has 26 connections,
 there are 1014x26 = 26364 connections from this Layer #1 to the prior 
layer. At this point, one of the benefits of a convolutional "shared 
weight" neural network should become more clear: because the weights are
 shared, even though there are 26364 connections, only 156 weights are 
needed to control those connections. As a consequence, only 156 weights 
need training. In comparison, a traditional "fully connected" neural 
network would have needed a unique weight for each connection, and would
 therefore have required training for 26364 different weights. None of 
that excess training is needed here.</p>
<p>Layer #2 is also a convolutional layer, but with 50 feature maps. 
Each feature map is 5x5, and each unit in the feature maps is a 5x5 
convolutional kernel of corresponding areas of all 6 of the feature maps
 of the previous layers, each of which is a 13x13 feature map. There are
 therefore 5x5x50 = 1250 neurons in Layer #2, (5x5+1)x6x50 = 7800 
weights, and 1250x26 = 32500 connections.</p>
<p>Before proceeding to Layer #3, it's worthwhile to mention a few 
points on the architecture of the neural network in general, and on 
Layer #2 in particular. As mentioned above, each feature map in Layer #2
 is connected to all 6 of the feature maps of the previous layer. This 
was a design decision, but it's not the only decision possible. As far 
as I can tell, the design is the same as Dr. Simard's design. But it's 
distinctly different from Dr. LeCun's design. Dr. LeCun deliberately 
chose not to connect each feature map in Layer #2 to all of the feature 
maps in the previous layer. Instead, he connected each feature map in 
Layer #2 to only a few selected ones of the feature maps in the previous
 layer. Each feature map was, in addition, connected to a different 
combination of feature maps from the previous layer. As Dr. LeCun 
explained it, his non-complete connection scheme would force the feature
 maps to extract different and (hopefully) complementary information, by
 virtue of the fact that they are provided with different inputs. One 
way of thinking about this is to imagine that you are forcing 
information through fewer connections, which should result in the 
connections becoming more meaningful. I think Dr. LeCun's approach is 
correct. However, to avoid additional complications to programming that 
was already complicated enough, I chose the simpler approach of Dr. 
Simard.</p>
<p>Other than this, the architectures of all three networks (i.e., the 
one described here and those described by Drs. LeCun and Simard) are 
largely similar.</p>
<p>Turning to Layer #3, Layer #3 is a fully-connected layer with 100 
units. Since it is fully-connected, each of the 100 neurons in the layer
 is connected to all 1250 neurons in the previous layer. There are 
therefore 100 neurons in Layer #3, 100*(1250+1) = 125100 weights, and 
100x1251 = 125100 connections.</p>
<p>Layer #4 is the final, output layer. This layer is a fully-connected 
layer with 10 units. Since it is fully-connected, each of the 10 neurons
 in the layer is connected to all 100 neurons in the previous layer. 
There are therefore 10 neurons in Layer #4, 10x(100+1) = 1010 weights, 
and 10x101 = 1010 connections.</p>
<p>Like Layer #2, this output Layer #4 also warrants an architectural 
note. Here, Layer #4 is implemented as a standard, fully connected 
layer, which is the same as the implementation of Dr. Simard's network. 
Again, however, it's different from Dr. LeCun's implementation. Dr. 
LeCun implemented his output layer as a "radial basis function", which 
basically measures the Euclidean distance between the actual inputs and a
 desired input (i.e., a target input). This allowed Dr. LeCun to 
experiment in tuning his neural network so that the output of Layer #3 
(the previous layer) matched idealized forms of handwritten digits. This
 was clever, and it also yields some very impressive graphics. For 
example, you can basically look at the outputs of his Layer #3 to 
determine whether the network is doing a good job at recognition. For my
 Layer #3 (and Dr. Simard's), the outputs of Layer #3 are meaningful 
only to the network; looking at them will tell you nothing. 
Nevertheless, the implementation of a standard, fully connected layer is
 far simpler than the implementation of radial basis functions, both for
 forward propagation and for training during backpropagation. I 
therefore chose the simpler approach.</p>
<p>Altogether, adding the above numbers, there are a total of 3215 
neurons in the neural network, 134066 weights, and 184974 connections.</p>
<p>The object is to train all 134066 weights so that, for an arbitrary 
input of a handwritten digit at the input layer, there is exactly one 
neuron at the output layer whose value is +1 whereas all other nine (9) 
neurons at the output layer have a value of -1. Again, the benchmark was
 an error rate of 0.82% or better, corresponding to the results obtained
 by Dr. LeCun.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="CodeToBuild"></a>
<h3>Code For Building the Neural Network</h3>
<p>The code for building the neural network is found in the <code>CMNistDoc::OnNewDocument()</code>
 function of the document class. Using the above illustration, together 
with its description, it should be possible to follow the code which is 
reproduced in simplified form below:</p><div style="display:block" width="100%" id="premain5" class="pre-action-link"><img preid="5" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg5" height="9" width="9"><span preid="5" style="cursor: pointer; margin-bottom: 0px;" id="precollapse5"> Collapse</span><span> | </span><a preid="5" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre5"><span class="code-comment">//</span><span class="code-comment"> simplified code
</span>
BOOL CMNistDoc::OnNewDocument()
{
    <span class="code-keyword">if</span> (!COleDocument::OnNewDocument())
        <span class="code-keyword">return</span> FALSE;
    
    <span class="code-comment">//</span><span class="code-comment"> grab the mutex for the neural network
</span>    
    CAutoMutex tlo( m_utxNeuralNet );
    
    <span class="code-comment">//</span><span class="code-comment"> initialize and build the neural net
</span>    
    NeuralNetwork&amp; NN = m_NN;  <span class="code-comment">//</span><span class="code-comment"> for easier nomenclature
</span>    NN.Initialize();
    
    NNLayer* pLayer;
    
    <span class="code-keyword">int</span> ii, jj, kk;
    <span class="code-keyword">double</span> initWeight;
    
    <span class="code-comment">//</span><span class="code-comment"> layer zero, the input layer.
</span>    <span class="code-comment">//</span><span class="code-comment"> Create neurons: exactly the same number of neurons as the input
</span>    <span class="code-comment">//</span><span class="code-comment"> vector of 29x29=841 pixels, and no weights/connections
</span>    
    pLayer = <span class="code-keyword">new</span> NNLayer( _T(<span class="code-string">"</span><span class="code-string">Layer00"</span>) );
    NN.m_Layers.push_back( pLayer );
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">841</span>; ++ii )
    {
        pLayer-&gt;m_Neurons.push_back( <span class="code-keyword">new</span> NNNeuron() );
    }

    
    <span class="code-comment">//</span><span class="code-comment"> layer one:
</span>    <span class="code-comment">//</span><span class="code-comment"> This layer is a convolutional layer that
</span>    <span class="code-comment">//</span><span class="code-comment"> has 6 feature maps.  Each feature 
</span>    <span class="code-comment">//</span><span class="code-comment"> map is 13x13, and each unit in the
</span>    <span class="code-comment">//</span><span class="code-comment"> feature maps is a 5x5 convolutional kernel
</span>    <span class="code-comment">//</span><span class="code-comment"> of the input layer.
</span>    <span class="code-comment">//</span><span class="code-comment"> So, there are 13x13x6 = 1014 neurons, (5x5+1)x6 = 156 weights
</span>    
    pLayer = <span class="code-keyword">new</span> NNLayer( _T(<span class="code-string">"</span><span class="code-string">Layer01"</span>), pLayer );
    NN.m_Layers.push_back( pLayer );
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">1014</span>; ++ii )
    {
        pLayer-&gt;m_Neurons.push_back( <span class="code-keyword">new</span> NNNeuron() );
    }
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">156</span>; ++ii )
    {
        initWeight = <span class="code-digit">0</span>.<span class="code-digit">05</span> * UNIFORM_PLUS_MINUS_ONE;
        <span class="code-comment">//</span><span class="code-comment"> uniform random distribution
</span>
        pLayer-&gt;m_Weights.push_back( <span class="code-keyword">new</span> NNWeight( initWeight ) );
    }
    
    <span class="code-comment">//</span><span class="code-comment"> interconnections with previous layer: this is difficult
</span>    <span class="code-comment">//</span><span class="code-comment"> The previous layer is a top-down bitmap
</span>    <span class="code-comment">//</span><span class="code-comment"> image that has been padded to size 29x29
</span>    <span class="code-comment">//</span><span class="code-comment"> Each neuron in this layer is connected
</span>    <span class="code-comment">//</span><span class="code-comment"> to a 5x5 kernel in its feature map, which 
</span>    <span class="code-comment">//</span><span class="code-comment"> is also a top-down bitmap of size 13x13. 
</span>    <span class="code-comment">//</span><span class="code-comment"> We move the kernel by TWO pixels, i.e., we
</span>    <span class="code-comment">//</span><span class="code-comment"> skip every other pixel in the input image
</span>    
    <span class="code-keyword">int</span> kernelTemplate[<span class="code-digit">25</span>] = {
        <span class="code-digit">0</span>,  <span class="code-digit">1</span>,  <span class="code-digit">2</span>,  <span class="code-digit">3</span>,  <span class="code-digit">4</span>,
        <span class="code-digit">29</span>, <span class="code-digit">30</span>, <span class="code-digit">31</span>, <span class="code-digit">32</span>, <span class="code-digit">33</span>,
        <span class="code-digit">58</span>, <span class="code-digit">59</span>, <span class="code-digit">60</span>, <span class="code-digit">61</span>, <span class="code-digit">62</span>,
        <span class="code-digit">87</span>, <span class="code-digit">88</span>, <span class="code-digit">89</span>, <span class="code-digit">90</span>, <span class="code-digit">91</span>,
        <span class="code-digit">116</span>,<span class="code-digit">117</span>,<span class="code-digit">118</span>,<span class="code-digit">119</span>,<span class="code-digit">120</span> };
        
    <span class="code-keyword">int</span> iNumWeight;
        
    <span class="code-keyword">int</span> fm;  <span class="code-comment">//</span><span class="code-comment"> "fm" stands for "feature map"
</span>        
    <span class="code-keyword">for</span> ( fm=0; fm&lt;<span class="code-digit">6</span>; ++fm)
    {
        <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">13</span>; ++ii )
        {
            <span class="code-keyword">for</span> ( jj=0; jj&lt;<span class="code-digit">13</span>; ++jj )
            {
                <span class="code-comment">//</span><span class="code-comment"> 26 is the number of weights per feature map
</span>                iNumWeight = fm * <span class="code-digit">26</span>;
                NNNeuron&amp; n = 
                   *( pLayer-&gt;m_Neurons[ jj + ii*13 + fm*169 ] );
                
                n.AddConnection( ULONG_MAX, iNumWeight++ );  <span class="code-comment">//</span><span class="code-comment"> bias weight
</span>                
                <span class="code-keyword">for</span> ( kk=0; kk&lt;<span class="code-digit">25</span>; ++kk )
                {
                    <span class="code-comment">//</span><span class="code-comment"> note: max val of index == 840, 
</span>                    <span class="code-comment">//</span><span class="code-comment"> corresponding to 841 neurons in prev layer
</span>                    n.AddConnection( 2*jj + 58*ii + 
                        kernelTemplate[kk], iNumWeight++ );
                }
            }
        }
    }
    
    
    <span class="code-comment">//</span><span class="code-comment"> layer two:
</span>    <span class="code-comment">//</span><span class="code-comment"> This layer is a convolutional layer
</span>    <span class="code-comment">//</span><span class="code-comment"> that has 50 feature maps.  Each feature 
</span>    <span class="code-comment">//</span><span class="code-comment"> map is 5x5, and each unit in the feature
</span>    <span class="code-comment">//</span><span class="code-comment"> maps is a 5x5 convolutional kernel
</span>    <span class="code-comment">//</span><span class="code-comment"> of corresponding areas of all 6 of the
</span>    <span class="code-comment">//</span><span class="code-comment"> previous layers, each of which is a 13x13 feature map
</span>    <span class="code-comment">//</span><span class="code-comment"> So, there are 5x5x50 = 1250 neurons, (5x5+1)x6x50 = 7800 weights
</span>    
    pLayer = <span class="code-keyword">new</span> NNLayer( _T(<span class="code-string">"</span><span class="code-string">Layer02"</span>), pLayer );
    NN.m_Layers.push_back( pLayer );
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">1250</span>; ++ii )
    {
        pLayer-&gt;m_Neurons.push_back( 
                 <span class="code-keyword">new</span> NNNeuron( (LPCTSTR)label ) );
    }
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">7800</span>; ++ii )
    {
        initWeight = <span class="code-digit">0</span>.<span class="code-digit">05</span> * UNIFORM_PLUS_MINUS_ONE;
        pLayer-&gt;m_Weights.push_back( <span class="code-keyword">new</span> NNWeight( initWeight ) );
    }
    
    <span class="code-comment">//</span><span class="code-comment"> Interconnections with previous layer: this is difficult
</span>    <span class="code-comment">//</span><span class="code-comment"> Each feature map in the previous layer
</span>    <span class="code-comment">//</span><span class="code-comment"> is a top-down bitmap image whose size
</span>    <span class="code-comment">//</span><span class="code-comment"> is 13x13, and there are 6 such feature maps.
</span>    <span class="code-comment">//</span><span class="code-comment"> Each neuron in one 5x5 feature map of this 
</span>    <span class="code-comment">//</span><span class="code-comment"> layer is connected to a 5x5 kernel
</span>    <span class="code-comment">//</span><span class="code-comment"> positioned correspondingly in all 6 parent
</span>    <span class="code-comment">//</span><span class="code-comment"> feature maps, and there are individual
</span>    <span class="code-comment">//</span><span class="code-comment"> weights for the six different 5x5 kernels.  As
</span>    <span class="code-comment">//</span><span class="code-comment"> before, we move the kernel by TWO pixels, i.e., we
</span>    <span class="code-comment">//</span><span class="code-comment"> skip every other pixel in the input image.
</span>    <span class="code-comment">//</span><span class="code-comment"> The result is 50 different 5x5 top-down bitmap
</span>    <span class="code-comment">//</span><span class="code-comment"> feature maps
</span>    
    <span class="code-keyword">int</span> kernelTemplate2[<span class="code-digit">25</span>] = {
        <span class="code-digit">0</span>,  <span class="code-digit">1</span>,  <span class="code-digit">2</span>,  <span class="code-digit">3</span>,  <span class="code-digit">4</span>,
        <span class="code-digit">13</span>, <span class="code-digit">14</span>, <span class="code-digit">15</span>, <span class="code-digit">16</span>, <span class="code-digit">17</span>, 
        <span class="code-digit">26</span>, <span class="code-digit">27</span>, <span class="code-digit">28</span>, <span class="code-digit">29</span>, <span class="code-digit">30</span>,
        <span class="code-digit">39</span>, <span class="code-digit">40</span>, <span class="code-digit">41</span>, <span class="code-digit">42</span>, <span class="code-digit">43</span>, 
        <span class="code-digit">52</span>, <span class="code-digit">53</span>, <span class="code-digit">54</span>, <span class="code-digit">55</span>, <span class="code-digit">56</span>   };
        
        
    <span class="code-keyword">for</span> ( fm=0; fm&lt;<span class="code-digit">50</span>; ++fm)
    {
        <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">5</span>; ++ii )
        {
            <span class="code-keyword">for</span> ( jj=0; jj&lt;<span class="code-digit">5</span>; ++jj )
            {
                <span class="code-comment">//</span><span class="code-comment"> 26 is the number of weights per feature map
</span>                iNumWeight = fm * <span class="code-digit">26</span>;
                NNNeuron&amp; n = *( pLayer-&gt;m_Neurons[ jj + ii*5 + fm*25 ] );
                
                n.AddConnection( ULONG_MAX, iNumWeight++ );  <span class="code-comment">//</span><span class="code-comment"> bias weight
</span>                
                <span class="code-keyword">for</span> ( kk=0; kk&lt;<span class="code-digit">25</span>; ++kk )
                {
                    <span class="code-comment">//</span><span class="code-comment"> note: max val of index == 1013,
</span>                    <span class="code-comment">//</span><span class="code-comment"> corresponding to 1014 neurons in prev layer
</span>                    n.AddConnection(       2*jj + 26*ii + 
                     kernelTemplate2[kk], iNumWeight++ );
                    n.AddConnection( <span class="code-digit">169</span> + 2*jj + 26*ii + 
                     kernelTemplate2[kk], iNumWeight++ );
                    n.AddConnection( <span class="code-digit">338</span> + 2*jj + 26*ii + 
                     kernelTemplate2[kk], iNumWeight++ );
                    n.AddConnection( <span class="code-digit">507</span> + 2*jj + 26*ii + 
                     kernelTemplate2[kk], iNumWeight++ );
                    n.AddConnection( <span class="code-digit">676</span> + 2*jj + 26*ii + 
                     kernelTemplate2[kk], iNumWeight++ );
                    n.AddConnection( <span class="code-digit">845</span> + 2*jj + 26*ii + 
                     kernelTemplate2[kk], iNumWeight++ );
                }
            }
        }
    }
            
    
    <span class="code-comment">//</span><span class="code-comment"> layer three:
</span>    <span class="code-comment">//</span><span class="code-comment"> This layer is a fully-connected layer
</span>    <span class="code-comment">//</span><span class="code-comment"> with 100 units.  Since it is fully-connected,
</span>    <span class="code-comment">//</span><span class="code-comment"> each of the 100 neurons in the
</span>    <span class="code-comment">//</span><span class="code-comment"> layer is connected to all 1250 neurons in
</span>    <span class="code-comment">//</span><span class="code-comment"> the previous layer.
</span>    <span class="code-comment">//</span><span class="code-comment"> So, there are 100 neurons and 100*(1250+1)=125100 weights
</span>    
    pLayer = <span class="code-keyword">new</span> NNLayer( _T(<span class="code-string">"</span><span class="code-string">Layer03"</span>), pLayer );
    NN.m_Layers.push_back( pLayer );
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">100</span>; ++ii )
    {
        pLayer-&gt;m_Neurons.push_back( 
           <span class="code-keyword">new</span> NNNeuron( (LPCTSTR)label ) );
    }
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">125100</span>; ++ii )
    {
        initWeight = <span class="code-digit">0</span>.<span class="code-digit">05</span> * UNIFORM_PLUS_MINUS_ONE;
    }
    
    <span class="code-comment">//</span><span class="code-comment"> Interconnections with previous layer: fully-connected
</span>    
    iNumWeight = <span class="code-digit">0</span>;  <span class="code-comment">//</span><span class="code-comment"> weights are not shared in this layer
</span>    
    <span class="code-keyword">for</span> ( fm=0; fm&lt;<span class="code-digit">100</span>; ++fm )
    {
        NNNeuron&amp; n = *( pLayer-&gt;m_Neurons[ fm ] );
        n.AddConnection( ULONG_MAX, iNumWeight++ );  <span class="code-comment">//</span><span class="code-comment"> bias weight
</span>        
        <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">1250</span>; ++ii )
        {
            n.AddConnection( ii, iNumWeight++ );
        }
    }
    
    <span class="code-comment">//</span><span class="code-comment"> layer four, the final (output) layer:
</span>    <span class="code-comment">//</span><span class="code-comment"> This layer is a fully-connected layer
</span>    <span class="code-comment">//</span><span class="code-comment"> with 10 units.  Since it is fully-connected,
</span>    <span class="code-comment">//</span><span class="code-comment"> each of the 10 neurons in the layer
</span>    <span class="code-comment">//</span><span class="code-comment"> is connected to all 100 neurons in
</span>    <span class="code-comment">//</span><span class="code-comment"> the previous layer.
</span>    <span class="code-comment">//</span><span class="code-comment"> So, there are 10 neurons and 10*(100+1)=1010 weights
</span>    
    pLayer = <span class="code-keyword">new</span> NNLayer( _T(<span class="code-string">"</span><span class="code-string">Layer04"</span>), pLayer );
    NN.m_Layers.push_back( pLayer );
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">10</span>; ++ii )
    {
        pLayer-&gt;m_Neurons.push_back( 
              <span class="code-keyword">new</span> NNNeuron( (LPCTSTR)label ) );
    }
    
    <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">1010</span>; ++ii )
    {
        initWeight = <span class="code-digit">0</span>.<span class="code-digit">05</span> * UNIFORM_PLUS_MINUS_ONE;
    }
    
    <span class="code-comment">//</span><span class="code-comment"> Interconnections with previous layer: fully-connected
</span>    
    iNumWeight = <span class="code-digit">0</span>;  <span class="code-comment">//</span><span class="code-comment"> weights are not shared in this layer
</span>    
    <span class="code-keyword">for</span> ( fm=0; fm&lt;<span class="code-digit">10</span>; ++fm )
    {
        NNNeuron&amp; n = *( pLayer-&gt;m_Neurons[ fm ] );
        n.AddConnection( ULONG_MAX, iNumWeight++ );  <span class="code-comment">//</span><span class="code-comment"> bias weight
</span>        
        <span class="code-keyword">for</span> ( ii=0; ii&lt;<span class="code-digit">100</span>; ++ii )
        {
            n.AddConnection( ii, iNumWeight++ );
        }
    }
    
    
    SetModifiedFlag( TRUE );
    
    <span class="code-keyword">return</span> TRUE;
}</pre>
<p>This code builds the illustrated neural network in stages, one stage for each layer. In each stage, an <code>NNLayer</code> is <code><span class="cpp-keyword">new</span></code>'d and then added to the <code>NeuralNetwork</code>'s vector of layers. The needed number of <code>NNNeuron</code>s and <code>NNWeight</code>s are <code><span class="cpp-keyword">new</span></code>'d and then added respectively to the layer's vector of neurons and vector of weights. Finally, for each neuron in the layer, <code>NNConnection</code>s are added (using the <code>NNNeuron::AddConnection()</code> function), passing in appropriate indices for weights and neurons.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="AboutMNist"></a>
<h2>MNIST Database of Handwritten Digits</h2>
<p>The MNIST database is modified (hence the "M") from a database of handwritten patterns offered by the <a href="http://www.nist.gov/srd/nistsd19.htm" target="_blank">National Institute of Standards and Technology ("NIST")</a>. As explained by Dr. LeCun at the <a href="http://yann.lecun.com/exdb/mnist/index.html" target="_newwin">MNIST section of his web site</a>,
 the database has been broken down into two distinct sets, a fist set of
 60,000 images of handwritten digits that is used as a training set for 
the neural network, and a second set of 10,000 images that is used as a 
testing set. The training set is composed of digits written by around 
250 different writers, of which approximately half are high school 
students and the other half are U.S. census workers. The number of 
writers in the testing set is unclear, but special precautions were made
 to ensure that all writers in the testing set were not also in the 
training set. This makes for a strong test, since the neural network is 
tested on images from writers that it has never before seen. It is thus a
 good test of the ability of the neural network to generalize, i.e., to 
extract intrinsically important features from the patterns in the 
training set that are also applicable to patterns that it has not seen 
before.</p>
<p>To use the neural network, you must download the MNIST database. 
Besides the two files that compose the patterns from the training and 
testing sets, there are also two companion files that give the 
"answers", i.e., the digit that is represented by a corresponding 
handwritten pattern. These two files are called "label" files. As 
indicated at the beginning of this article, the four files can be 
downloaded from <a href="http://yann.lecun.com/exdb/mnist/index.html" target="_blank">here (11,594 Kb total)</a>.</p>
<p>Incidentally, it's been mentioned that Dr. LeCun's achieval of an 
error rate of 0.82% has been used as a benchmark. If you read Dr. 
Simard's article, you will see that he claims an even better error rate 
of 0.40%. Why not use Dr. Simard's 0.40% as the benchmark?</p>
<p>The reason is that Dr. Simard did not respect the boundary between 
the training set and the testing set. In particular, he did not respect 
the fact that the writers in the training set were distinct from the 
writers in the testing set. In fact, Dr. Simard did not use the testing 
set at all. Instead, he trained with the first 50,000 patterns in the 
training set, and then tested with the remaining 10,000 patterns. This 
raises the possibility that, during testing, Dr. Simard's network was 
fed patterns from writers whose handwriting had already been seen 
before, which would give the network an unfair advantage. Dr. LeCun, on 
the other hand, took pains to ensure that his network was tested with 
patterns from writers it had never seen before. Thus, as compared with 
Dr. Simard's testing, Dr. LeCun's testing was more representative of 
real-world results since he did not give his network any unfair 
advantages. That's why I used Dr. LeCun's error rate of 0.82% as the 
benchmark.</p>
<p>Finally, you should note that the MNIST database is still widely used
 for study and testing, despite the fact that it was created back in 
1998. As one recent example, published in February 2006, see:</p>
<ul>
<li>Fabien Lauer, Ching Y. Suen and Gerard Bloch, <a href="http://hal.archives-ouvertes.fr/docs/00/05/75/61/PDF/LauerSuenBlochPR.pdf" target="_newwin">"A Trainable Feature Extractor for Handwritten Digit Recognition"</a>, Elsevier Science, February 2006 </li></ul>
<p>In the Lauer et al. article, the authors used a convolutional neural 
network for everything except the actual classification/recognition 
step. Instead, they used the convolutional neural network for black-box 
extraction of feature vectors, which they then fed to a different type 
of classification engine, namely a support vector machine ("SVM") 
engine. With this architecture, they were able to obtain the excellent 
error rate of just 0.54%. Good stuff.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Architecture"></a>
<h2>Overall Architecture of the Test/Demo Program</h2>
<p>The test program is an MFC SDI doc/view application, using worker threads for the various neural network tasks.</p>
<p>The document owns the neural network as a protected member variable. 
Weights for the neural network are saved to and loaded from an <code>.nnt</code> file in the <code>CMNistDoc::Serialize()</code> function. Storage/retrieval of the weights occurs in response to the menu items "<code>File-&gt;Open</code>" and "<code>File-&gt;Save</code>" or "<code>Save As</code>". For this purpose, the neural network also has a <code>Serialize()</code>
 function, which was not shown in the simplified code above, and it is 
the neural network that does the heavy lifting of storing its weights to
 a disk file, and extracting them later.</p>
<p>The document further holds two static functions that are used for the
 worker threads to run backpropagation and testing on the neural 
network. These functions are unimaginatively named <code>CMNistDoc::BackpropagationThread()</code> and <code>CMNistDoc::TestingThread()</code>.
 The functions are not called directly; instead, other classes that wish
 to begin a backpropagation or testing cycle ask the document to do so 
for them, by calling the document's <code>StartBackpropagation()</code> and <code>StartTesting()</code>
 functions. These functions accept parameters for the backpropagation or
 the testing, and then kick off the threads to do the work. Results are 
reported back to the class that requested the work using the API's <code>::PostMessage()</code> function with a user-defined message and user-defined codings of the <code>wParam</code> and <code>lParam</code> parameters.</p>
<p>Two helper classes are also defined in the document, named <code>CAutoCS</code> and <code>CAutoMutex</code>.
 These are responsible for automatic locking and release of critical 
sections and mutexes that are used for thread synchronization. Note that
 the neural network itself does not have a critical section. I felt that
 thread synchronization should be the responsibility of the owner of the
 neural network, and not the responsibility of the neural network, which
 is why the document holds these synchronization objects. The <code>CAutoCS</code> and <code>CAutoMutex</code>
 classes lock and release the synchronization objects through use of 
well-known RAII techniques ("resource acquisition is initialization").</p>
<p>The document further owns the MNIST database files, which are opened and closed in the <code>CMNistDoc::OnButtonOpenMnistFiles()</code> and <code>CMNistDoc::OnButtonCloseMnistFiles()</code>
 functions. These functions are button handlers for 
correspondingly-labeled buttons on the view. When opening the MNIST 
files, all four files must be opened, i.e., the pattern files for the 
training and testing sets, and the label files for the training and 
testing set. There are therefore four "<code>Open File</code>" dialogs, and prompts on the dialogs tell you which file should be opened next.</p>
<p>The view is based on <code>CFormView</code>, and holds a single tab 
control with three tabs, a first for a graphical view of the neural 
network and the outputs of each of its neurons, a second for 
backpropagation and training, and a third for testing. The view is 
resizable, using an excellent class named <code>DlgResizeHelper</code>, described at <a href="http://www.codeguru.com/Cpp/W-D/dislog/resizabledialogs/article.php/c1913/" target="_newwin">Dialog Resize Helper</a> by Stephan Keil.</p>
<p>Each tab on the tab control hosts a different <code>CDialog</code>-derived class. The first tab hosts a <code>CDlgCharacterImage</code> dialog which provides the graphical view of the neural network and the outputs of each neuron. The second tab hosts a <code>CDlgNeuralNet</code>
 dialog which provides control over training of the neural network, and 
which also gives feedback and progress during the training process 
(which can be lengthy). The third tab hosts a <code>CDlgTesting</code> 
dialog which provides control over testing and outputs test results. 
Each of these tabs/dialogs is described in more detail in the following 
sections.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Using"></a>
<h3>Using The Demo Program</h3>
<p>To use the program, you must open <b><i>five</i></b> files: four files comprising the MNIST database, and one file comprising the weights for the neural network.</p>
<p>To open the MNIST database files, click the "Open MNist" button at 
the bottom of the screen. You will be prompted four successive times to 
open the four files comprising the MNIST database, namely, the training 
patterns, the label (i.e., the answers) for the training patterns, the 
testing patterns, and the labels for the testing patterns.</p>
<p>To open the weights for the neural network, from the main menu, 
choose File-&gt;Open, or choose the file-open icon from the toolbar. You
 will be prompted to open a <i>.nnt</i> file that contains the weights and the structure of the neural network.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="GraphicalView"></a>
<h3>Graphical View of the Neural Network</h3>
<p>The graphical view of the neural network is the same as the screenshot at the top of this article, and it's repeated again here:</p><br><img alt="Graphical view of the neural network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Screenshot-GraphicalView.gif" border="0" height="372" width="600"> 
<p>The window at the mid-right shows the output of <i><b>all</b></i> 
3215 neurons. For each neuron, the output is represented as a single 
grayscale pixel whose gray level corresponds to the neuron's output: 
white equals a fully off neuron (value of -1) and black equals a fully 
on neuron (value of +1). The neurons are grouped into their layers. The 
input layer is the 29x29 pattern being recognized. Next are the six 
13x13 feature maps of the second layer, followed in turn by the 50 5x5 
feature maps of the third layer, the 100 neurons of the fourth layer 
(arranged in a column), and finally the 10 output neurons in the output 
layer.</p>
<p>Since individual pixels are hard to see clearly, a magnified view is 
provided. Simply move your mouse over the window, and a magnified window
 is displayed that lets you see each pixel's value clearly.</p>
<p>The display for the output layer is a bit different than for the 
other layers. Instead of a single pixel, the 10 output neurons are shown
 as a grayscale representation of the digit coded by that neuron. If the
 neuron is fully off (value of -1), the digit is shown as pure white, 
which of course means that it can't be seen against the white 
background. If the neuron is fully on (value of +1) then the digit is 
shown in pure black. In-between values are shown by varying degrees of 
grayness. In the screenshot above, the output of the neural network 
shows that it firmly believes that the pattern being recognized is an 
"8" (which is correct). But the dimly displayed "2" also shows that the 
neural network sees at least some similarity in the pattern to the digit
 "2". A red marker points to the most likely result.</p>
<p>The mid-left of the dialog shows the appearance of the pattern at a 
selectable index. Here the pattern's index is 1598 in the training set. 
Controls are arranged to allow fast sequencing through the patterns, and
 to allow selection of either the training set or the testing set. There
 is also a check box for distortion of the pattern. Distortion helps in 
training of the neural network, since it forces the network to extract 
the intrinsic shapes of the patterns, rather than allowing the network 
to (incorrectly) focus on peculiarities of individual patterns. This is 
discussed in greater detail below, but in essence, it helps the neural 
network to generalize. If you look carefully at the screenshot above, 
you will notice that the input pattern for the digit "8" (on the left) 
has been slightly distorted when it is given as the input layer to the 
neural network (on the right).</p>
<p>The "Calculate" button causes the neural network to forward propagate
 the pattern in the input layer and attempt a recognition of it. The 
actual numeric values of the output neurons are shown in the center, 
together with the time taken for the calculation. Although almost all 
other operations on the neural network are performed in a thread, for 
recognitions of single patterns, the time taken is so short (typically 
around 15 milliseconds) that the calculation is performed in the main UI
 thread.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="TrainingView"></a>
<h3>Training View and Control Over the Neural Network</h3>
<p>Unless training is actually ongoing, the training view hides many of 
its controls. Training is started by clicking on the "Start 
Backpropagation" button, and during training, the training view looks 
like the following screenshot:</p>
<p><img alt="Training view of the neural network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Screenshot-TrainingView.gif" border="0" height="370" width="600"></p>
<p>During training, the 60,000 patterns in the training set are 
continuously processed (in a random sequence) by the backpropagation 
algorithm. Each pass through the 60,000 patterns is called an "epoch". 
The display shows statistics about this process. Starting at the top, 
the display gives an estimate of the current mean squared error ("MSE") 
of the neural network. MSE is the value of <img alt="Error due to a single pattern P at the last layer n" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ErrNP.gif" align="middle" border="0" height="25" width="25">
 in equation (1) above, averaged across all 60,000 patterns. The value 
at the top is only a running estimate, calculated over the last 200 
patterns.</p>
<p>A history of the current estimate of MSE is seen graphically just to 
the right. This is a graph of the current estimates of MSE, over the 
last four epochs.</p>
<p>Just below are a few more statistics, namely the number of 
fully-completed epochs, and the cardinal number of the current pattern 
being backpropagated. A progress bar is also shown.</p>
<p>Below that is an edit control showing information about each epoch. The information is updated as each epoch is completed.</p>
<p>The edit control also indicates when the Hessian (needed for second 
order backpropagation) is being calculated. During this time, which 
requires an analysis of 500 random patters, the neural network is not 
able to do much else, so a string of dots slowly scrolls across the 
control. Frankly, it's not logical to put this display inside the edit 
control, and this might change in future developments.</p>
<p>Backpropagation can be stopped at any time by clicking the "Stop Backpropagation" button.</p>
<p><a href="#topmost"><small>go back to top</small></a> <a name="TestingView"></a>
</p><h3>Testing View of the Neural Network</h3>
<p>The testing view of the neural network is shown in the following screenshot:</p>
<p><img alt="Testing view of the neural network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Screenshot-TestingView.gif" border="0" height="372" width="600"> 
</p><p>In the testing mode, the 10,000 patterns in the testing set are 
run through the neural network, and if the output of the neural network 
does not match the expected output, an error is recorded in the view's 
main edit control. A progress bar through the testing set is displayed, 
and at the completion of testing, a summary of the total number of 
errors is displayed.</p>
<p>A testing sequence is commenced by clicking the "Start Testing" 
button, at which point you are shown a dialog from which you can select 
the testing parameters:</p>
<p><img alt="Selection of testing parameters" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Screenshot-TestingParameters.gif" border="0" height="189" width="282"></p>
<p>It's usually best simply to select the default values. The meaning of
 the available parameters will become clearer once we look at the 
training parameters in the next section. Note that it is possible to 
"test" the training set, which can be useful for checking on convergence
 of the weights.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Training"></a>
<h2>Training the Neural Network</h2>
<p>In order of importance, training is probably the next most important 
topic after design of the overall architecture of the neural network. We
 will discuss it over the next few sections.</p>
<p>The idea of training is to force the neural network to generalize. 
The word "generalization" deserves a few moments of your thought. An 
enormous amount of time is spent in analyzing the training set, but out 
in the real world, the performance of the neural network on the training
 set is of absolutely no interest. Almost any neural network can be 
trained so well it might not encounter any errors at all on the training
 set. But who cares about that? We already know the answers for the 
training set. The real question is, how well will the neural network 
perform out in the real world on patterns that it has never before seen?</p>
<p>The ability of a neural network to do well on patterns that it has 
never seen before is called "generalization". The effort spent in 
training is to force the neural network to see intrinsic characteristics
 in the training set, which we hope will also be present in the real 
world. When we calculate the MSE of the training set, we do so with the 
understanding that this is only an estimate of the number that we are 
really interested in, namely, the MSE of real-world data.</p>
<p>A few techniques are used during training in an effort to improve the
 neural network's generalization. As one example, the training set is 
passed through the neural network in a randomized order in each epoch. 
Conceptually, these techniques cause the weights to "bounce around" a 
bit, thereby preventing the neural network from settling on weights that
 emphasize false attributes of the patterns, i.e., attributes that are 
peculiar to patterns in the training set but which are not intrinsic 
characteristics of patterns encountered out in the real world.</p>
<p>One of the more important techniques for improving generalization is 
distortion: the training patterns are distorted slightly before being 
used in backpropagation. The theory is that these distortions force the 
neural network to look more closely at the important aspects of the 
training patters, since these patterns will be different each time the 
neural network sees them. Another way of looking at this is that 
distortions enlarge the size of the training set, by artificially 
creating new training patterns from existing ones.</p>
<p>In this program, the <code>CMNistDoc::GenerateDistortionMap()</code> function generates a distortion map that is applied to the training patterns (using the <code>CMNistDoc::ApplyDistortionMap()</code>
 function) during training. The distortion is calculated randomly for 
each pattern just before backpropagation. Three different types of 
distortions are applied:</p>
<ul>
<li>Scale Factor: The scale of the pattern is changed so as to enlarge 
or shrink it. The scale factor for the horizontal direction is different
 from the scale factor for the vertical direction, such that it is 
possible to see shrinkage in the vertical scale and enlargement in the 
horizontal scale. 
</li><li>Rotation: The entire pattern is rotated clockwise or counterclockwise. 
</li><li>Elastic: This is a word borrowed Dr. Simard's <a href="http://research.microsoft.com/%7Epatrice/PDF/fugu9.pdf" target="_newwin">"Best Practices"</a>
 paper. Visually, elastic distortions look like a gentle pushing and 
pulling of pixels in the pattern, almost like viewing the pattern 
through wavy water. </li></ul>
<p>The result of distortions is illustrated in the following diagram, 
which shows the original of a pattern for the digit "3" together with 25
 examples of distortions applied to the pattern:</p>
<p><img alt="Examples of distortions applied to training patterns" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/DistortionCollage.gif" border="0" height="148" width="180"> 
</p><p>You can see the effect of the three types of distortions, but to a
 human viewer, it is clear that each distortion still "looks like" the 
digit "3". To the neural network, however, the pixel patterns are 
remarkably different. Because of these differences, the neural network 
is forced to see beyond the actual pixel patterns, and hopefully to 
understand and apply the intrinsic qualities that make a "3" look like a
 "3".</p>
<p>You select whether or not to apply distortions when you click the 
"Start Backpropagation" button in the training view, at which point you 
are shown the following dialog which lets you select other training 
parameters as well:</p>
<p><img alt="Selection of training parameters" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Screenshot-BackpropagationParameters.gif" border="0" height="230" width="282"> 
</p><p>Besides distortions, this dialog lets you set other parameters 
for backpropagation. Most notably, you must specify an initial learning 
rate "<i>eta</i>" and a final learning rate. Generally speaking, the 
learning rate should be larger for untrained neural networks, since a 
large learning rate will allow large changes in the weights during 
backpropagation. The learning rate is slowly decreased during training, 
as the neural network learns, so as to allow the weights to converge to 
some final value. But it never really makes sense to allow the learning 
rate to decrease too much, or the network will stop learning. That's the
 purpose of the entry for the final value.</p>
<p>For the initial value of the learning rate, never use a value larger 
than around 0.001 (which is the default). Larger values cause the 
weights to change too quickly, and actually cause the weights to <i><b>diverge</b></i> rather than converge.</p>
<p>Most of the literature on training of neural networks gives a 
schedule of learning rates as a function of epoch number. Dr. LeCun, for
 example, recommends a schedule of 0.0005 for two epochs, followed by 
0.0002 for the next three, 0.0001 for the next three, 0.00005 for the 
next four, and 0.00001 thereafter. I found it easier to program a 
reduction factor that is applied after N recognitions. Frankly, I think 
it might have been better to listen to the experts, and this might 
change in the future to a schedule of learning rates. At present, 
however, at the end of N recognitions (such as 120,000 recognitions 
which corresponds to two epochs and is the default value), the program 
simply multiplies the current learning rate by a factor that's less than
 one, which results in a continuously decreasing learning rate.</p>
<p>The dialog was also written at a naive time of development, when I 
thought it might be possible to reduce the learning rate within a single
 epoch, rather than only after an epoch is completed. So, the reduction 
factor is given in terms of a reduction after N recognitions, rather 
than after N epochs which would have been more appropriate.</p>
<p>The default values for the dialog are values that I found to work 
well for untrained networks. The initial learning rate is 0.001, which 
is rather large and results in good coarse training quickly, 
particularly given the second order nature of the backpropagation 
algorithm. The learning rate is reduced by 79.4% of its value after 
every two epochs. So, for example, the training rate for the third epoch
 is 0.000794. The final learning rate is 0.00005, which is reached after
 around 26 epochs. At this point the neural network is well-trained.</p>
<p>The other options in the dialog are discussed below in the "Tricks" section.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Tricks"></a>
<h2>Tricks That Make Training Faster</h2>
<p>As indicated in the beginning of the article, this project is an 
engineering work-in-progress, where many different adjustments are made 
in an effort to improve accuracy. Against that background, training can 
be a maddeningly slow process, where it takes several CPU hours to 
determine whether or not a change was beneficial. As a consequence, 
there were a few things done to make training progress more quickly.</p>
<p>Three things in particular were done: second order backpropagation, 
multithreading, and backpropagation skipping. The first has been done 
before, but I have not seen the second and third techniques in any of 
the resources that I found (maybe they're new??). Each is discussed 
below.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Hessian"></a>
<h3>Second Order Backpropagation Using Pseudo-Hessian</h3>
<p>Second order backpropagation is not really a "trick"; it's a 
well-understood mathematical technique. And it was discussed above in 
the section on "<a href="#SecondOrder">Second Order Methods</a>". It's 
mentioned here again since it clearly had the greatest impact on 
speeding convergence of the neural network, in that it dramatically 
reduced the number of epochs needed for convergence of the weights.</p>
<p>The next two techniques are slightly different in emphasis, since 
they reduce the amount of time spent in any one epoch by speeding 
progress through the epoch. Put another way, although second order 
backpropagation reduces the overall time needed for training, the next 
two techniques reduce the time needed to go through all of the 60,000 
patterns that compose a single epoch.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="SimultaneousBackprop"></a>
<h3>Simultaneous Backpropagation and Forward Propagation</h3>
<p>In this technique, multiple threads are used to speed the time needed for one epoch.</p>
<p>Note that in general, multiple threads will almost always have a <b><i>negative</i></b>
 impact on performance. In other words, if it takes time T to perform 
all work in a single thread, then it will take time T+x to perform the 
same work with multiple threads. The reason is that the overhead of 
context switching between threads, and the added burden of 
synchronization between threads, adds time that would not be necessary 
if only a single thread were used.</p>
<p>The only circumstance where multiple threads will improve performance
 is where there are also multiple processors. If there are multiple 
processors, then it makes sense to divide the work amongst the 
processors, since if there were only a single thread, then all the other
 processors would wastefully be idle.</p>
<p>So, this technique helps only if your machine has multiple 
processors, i.e., if you have a hyper-threaded or dual core processor 
(Intel terminology) or dual processors (AMD terminology). If you do not 
have multiple processors, then you will not see any improvement when 
using multiple threads. Moreover, there is absolutely no benefit to 
running more threads than you have processors. So, if you have a 
hyperthreaded machine, use two threads exactly; use of three or more 
threads does not give any advantage.</p>
<p>But there's a significant problem in trying to implement a 
multi-threaded backpropagation algorithm. The problem is that the 
backpropagation algorithm depends on the numerical outputs of the 
individual neurons from the forward propagation step. So consider a 
simple multi-threaded scenario: a first thread forward propagates a 
first input pattern, resulting in neuron outputs for each layer of the 
neural network including the output layer. The backpropagation stage is 
then started, and equation (2) above is used to calculate how errors in 
the pattern depend on outputs of the output layer.</p>
<p>Meanwhile, because of multithreading, another thread selects a second
 pattern and forward propagates it through the neural network, in 
preparation for the backpropagation stage. There's a context switch back
 to the first thread, which now tries to complete backpropagation for 
the first pattern.</p>
<p>And here's where the problem becomes evident. The first thread needs 
to apply equations (3) and (4). But these equations depend on the 
numerical outputs of the neurons, and <b><i>all</i></b> of those values have now changed, because of the action of the second thread's forward propagation of the second pattern.</p>
<p>My solution to this dilemma is to memorize the outputs of all the 
neurons. In other words, when forward propagating an input pattern, the 
outputs of all 3215 neurons are stored in separate memory locations, and
 these memorized outputs are used during the backpropagation stage. 
Since the outputs are stored, another thread's forward propagation of a 
different pattern will not affect the backpropagation calculations.</p>
<p>The sequence runs something like this. First, a thread tries to lock a
 mutex that protects the neural network, and will wait until it can 
obtain the lock. Once the lock is obtained, the thread forward 
propagates a pattern and memorizes the output values of all neurons in 
the network. The mutex is then released, which allows another thread to 
do the same thing.</p>
<p>Astute readers will note that this solution might solve the problem 
of equations (2) through (4), since those equations depend on the 
numerical values of the neuron outputs, but that this solution does not 
address additional problems caused by equations (5) and (6). These two 
equations depend on the values of the weights, and the values of the 
weights might be changed out from under one thread by another thread's 
backpropagation. For example, continuing the above explanation, the 
first thread starts to backpropagate, and changes the values of the 
weights in (say) the last two layers. At this point, there is a context 
switch to the second thread, which also tries to calculate equation (5) 
and also tries to change the values of weights according to equation 
(6). But since both equations depend on weights that have now been 
changed by the first thread, the calculations performed by the second 
thread are not valid.</p>
<p>The solution here is to ignore the problem, and explain why it's 
numerically valid to do so. It's valid to ignore the problem because the
 errors caused by using incorrect values for the weights are generally 
negligible. To see that the errors are negligible, let's use a numeric 
example. Consider a worst-case situation that might occur during very 
early training when the errors in the output layer are large. How large 
can the errors be? Well, the target value for the output of a neuron 
might be +1, whereas the actual output might be -1 which is as wrong as 
it can get. So, per equation (2), the error is 2. Let's backpropagate 
that error of 2. In equation (3) we see that the error is multiplied by <i>G(x)</i>, which is the derivative of the activation function. The maximum value that <i>G(x)</i>
 can assume is around +1, so the error of 2 is still 2. Likewise, in 
equation (4), the value of 2 is multiplied by the outputs of neurons in 
the previous layer. But those outputs generally cannot exceed a value of
 +1 (since the outputs are limited by the activation function). So 
again, the error of 2 is still 2.</p>
<p>We need to skip equation (5) for a moment, since it's equation (6) 
that actually changes the values of the weights. In equation (6) we see 
that the change in the weight is our value of 2 multiplied by the 
learning rate <i>eta</i> (possibly multiplied by a second order Hessian). We know <i>eta</i>
 is small, since we deliberately set it to a small value like 0.001 or 
less. So, any weight might be changed by as much as our value of 2 
multiplied by <i>eta</i>, or 2 x 0.001 = 0.002.</p>
<p>Now we can tackle equation (5). Let's assume that the second thread 
has just modified a weight as described above, which is now 0.002 
different from the value that the first thread should be using. The 
answer here is a big "so what". It's true that the value of the weight 
has been changed, but the change is only 0.2%. So even though the first 
thread is now using a value for the weight that is wrong, the amount by 
which the weight is wrong is completely negligible, and it's justifiable
 to ignore it.</p>
<p>It can also be seen that the above explanation is a worst-case 
explanation. As the training of the neural network improves, the errors 
in each layer are increasingly smaller, such that it becomes even more 
numerically justifiable to ignore this effect of multithreading.</p>
<p>There's one final note on implementation in the code. Since the 
program is multithreaded, it cannot alter weights without careful 
synchronization with other threads. The nature of equation (6) is also 
such that one thread must be respectful of changes to the value of 
weight made by other threads. In other words, if the first thread simply
 wrote over the value of the weight with the value it wants, then 
changes made by other threads would be lost. In addition, since the 
weights are stored as 64-bit <code><span class="cpp-keyword">double</span></code>s,
 there would be a chance that the actual weight might be corrupt if a 
first thread were interrupted before it had a chance to update both of 
the 32-bit halves of the weight.</p>
<p>In this case, synchronization is achieved without a kernel-mode 
locking mechanism. Rather, synchronization is achieved with a 
compare-and-swap ("CAS") approach, which is a lock-free approach. This 
approach makes use of atomic operations that are guaranteed to complete 
in one CPU cycle. The function used is <code>InterlockedCompareExchange64()</code> which performs an atomic compare-and-exchange operation on specified 64-bit values. The function compares a <i>destination</i> value with a <i>comparand</i> value and exchanges it with an <i>exchange</i>
 value only if the two are equal. If the two are not equal, then that 
means that another thread has modified the value of the weight, and the 
code tries again to make another exchange. Theoretically (and unlike 
locking mechanisms for synchronization), lock-free methods are not 
guaranteed to complete in finite time, but that's theory only and in 
practice lock-free mechanisms are widely used.</p>
<p>The code for updating the weight is therefore the following, which can be found in the <code>NNLayer::Backpropagate()</code> function. Note that the code given above in the "<a href="#Backpropagation">Backpropagation</a>" section was simplified code that omitted this level of detail in the interests of brevity:</p><div style="display:block" width="100%" id="premain6" class="pre-action-link"><img preid="6" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg6" height="9" width="9"><span preid="6" style="cursor: pointer; margin-bottom: 0px;" id="precollapse6"> Collapse</span><span> | </span><a preid="6" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre6"><span class="code-comment">//</span><span class="code-comment"> simplified code showing an excerpt
</span><span class="code-comment">//</span><span class="code-comment"> from the NNLayer::Backpropagate() function
</span>
<span class="code-comment">//</span><span class="code-comment"> finally, update the weights of this layer
</span><span class="code-comment">//</span><span class="code-comment"> neuron using dErr_wrt_dW and the learning rate eta
</span><span class="code-comment">//</span><span class="code-comment"> Use an atomic compare-and-exchange operation,
</span><span class="code-comment">//</span><span class="code-comment"> which means that another thread might be in 
</span><span class="code-comment">//</span><span class="code-comment"> the process of backpropagation
</span><span class="code-comment">//</span><span class="code-comment"> and the weights might have shifted slightly
</span>
<span class="code-keyword">struct</span> DOUBLE_UNION
{
    union 
    {
        <span class="code-keyword">double</span> dd;
        unsigned __int64 ullong;
    };
};

DOUBLE_UNION oldValue, newValue;

<span class="code-keyword">double</span> epsilon, divisor;

<span class="code-keyword">for</span> ( jj=0; jj&lt;m_Weights.size(); ++jj )
{
    divisor = <span class="code-comment">/*</span><span class="code-comment"> a value that depends on the second order Hessian */</span> 
    
    epsilon = etaLearningRate / divisor;
    <span class="code-comment">//</span><span class="code-comment"> amplify the learning rate based on the Hessian
</span>
    oldValue.dd = m_Weights[ jj ]-&gt;value;
    newValue.dd = oldValue.dd - epsilon * dErr_wrt_dWn[ jj ];
        
    <span class="code-keyword">while</span> ( oldValue.ullong != _InterlockedCompareExchange64( 
           (unsigned __int64*)(&amp;m_Weights[ jj ]-&gt;value), 
            newValue.ullong, oldValue.ullong ) ) 
    {
        <span class="code-comment">//</span><span class="code-comment"> another thread must have modified the weight.
</span>        <span class="code-comment">//</span><span class="code-comment"> Obtain its new value, adjust it, and try again
</span>        
        oldValue.dd = m_Weights[ jj ]-&gt;value;
        newValue.dd = oldValue.dd - epsilon * dErr_wrt_dWn[ jj ];
    }
    
}</pre>
<p>I was very pleased with this speed-up trick, which showed dramatic 
improvements in speed of backpropagation without any apparent adverse 
effects on convergence. In one test, the machine had an Intel Pentium 4 
hyperthreaded processor, running at 2.8gHz. In singly threaded usage, 
the machine was able to backpropagate at around 13 patterns per second. 
In two-threaded use, backpropagation speed was increased to around 18.6 
patterns per second. That's an improvement of around 43% in 
backpropagation speed, which translates into a reduction of 30% in 
backpropagation time.</p>
<p>In another test, the machine had an AMD Athlon 64 x2 Dual Core 4400+ 
processor, running at 2.21 gHz. For one-threaded backpropagation the 
speed was around 12 patterns per second, whereas for two-threaded 
backpropagation the speed was around 23 patterns per second. That's a 
speed improvement of 92% and a corresponding reduction of 47% in 
backpropagation time.</p>
<p>As one final implementation note, my development platform is VC++ 6.0, which does not have the <code>InterlockedCompareExchange64()</code> function, not even as a compiler intrinsic. With help from the comp.programming.threads newsgroup (see <a href="http://groups.google.com/group/comp.programming.threads/browse_thread/thread/1c3b38cd249ff2ba/e90ff2c919f84612" target="_newwin">this thread</a>), I wrote my own assembler version, which I am reproducing here:</p><div style="display:block" width="100%" id="premain7" class="pre-action-link"><img preid="7" style="cursor: pointer;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/minus.gif" id="preimg7" height="9" width="9"><span preid="7" style="cursor: pointer; margin-bottom: 0px;" id="precollapse7"> Collapse</span><span> | </span><a preid="7" href="#">Copy Code</a></div><pre style="margin-top: 0px;" id="pre7">inline unsigned __int64 
_InterlockedCompareExchange64(<span class="code-keyword">volatile</span> unsigned __int64 *dest,
                           unsigned __int64 exchange,
                           unsigned __int64 comparand) 
{
    <span class="code-comment">//</span><span class="code-comment">value returned in eax::edx
</span>    __asm {
        lea esi,comparand;
        lea edi,exchange;
        
        mov eax,[esi];
        mov edx,<span class="code-digit">4</span>[esi];
        mov ebx,[edi];
        mov ecx,<span class="code-digit">4</span>[edi];
        mov esi,dest;
        <span class="code-comment">//</span><span class="code-comment">lock CMPXCHG8B [esi] is equivalent to the following except
</span>        <span class="code-comment">//</span><span class="code-comment">that it's atomic:
</span>        <span class="code-comment">//</span><span class="code-comment">ZeroFlag = (edx:eax == *esi);
</span>        <span class="code-comment">//</span><span class="code-comment">if (ZeroFlag) *esi = ecx:ebx;
</span>        <span class="code-comment">//</span><span class="code-comment">else edx:eax = *esi;
</span>        <span class="code-keyword">lock</span> CMPXCHG8B [esi];            
    }
}</pre>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="SkipBackprop"></a>
<h3>Skip Backpropagation for Small Errors</h3>
<p>For a decently-trained network, there are some patterns for which the
 error is very small. The idea of this speed-up trick, therefore, is 
that for such patterns where there is only a small error, it is frankly 
meaningless to backpropagate the error. The error is so small that the 
weights won't change. Simply put, for errors that are small, there are 
bigger fish to fry, and there's no point in spending time to 
backpropagate small errors.</p>
<p>In practice, the demo program calculates the error for each pattern. 
If the error is smaller than some small fraction of the current MSE, 
then backpropagation is skipped entirely, and the program proceeds 
immediately to the next pattern.</p>
<p>The small fraction is set empirically to one-tenth of the current 
MSE. For a pattern whose error is smaller than one-tenth the overall MSE
 in the prior epoch, then backpropagation is skipped entirely, and 
training proceeds to the next pattern.</p>
<p>The actual value of the fractional threshold is settable in the program's <i>.ini</i>
 file. The dialog for setting training parameters asks you to input the 
current MSE, so that it has some idea of the cut-off point between 
performing and not performing backpropagation. It advises you to input a
 small number for the MSE if you are unsure of the actual MSE, since for
 small MSE's, essentially all errors are backpropagated.</p>
<p>In testing, as the neural network gets increasingly well-trained, I 
have found that approximately one-third of the patterns result in an 
error so small that there's no point in backpropagation. Since each 
epoch is 60,000 patterns, there's no backpropagation of error for around
 20,000 patterns. The remaining 40,000 patterns result in an error large
 enough to justify backpropagation.</p>
<p>On an Intel Pentium 4 hyperthreaded processor running at 2.8gHz, when
 these two speed-up tricks are combined (i.e., the multithreading 
speed-up, combined with skipping of backpropagations for patterns that 
result in a small error), each epoch completes in around 40 minutes. 
It's still maddeningly slow, but it's not bad.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Experiences"></a>
<h2>Experiences In Training the Neural Network</h2>
<p>The default values for parameters found on the "Training" dialog are 
the result of my experiences in training the neural network for best 
performance on the testing set. This section gives an abbreviated 
history of those experiences.</p>
<p>Note: I only recently realized that I should have been tracking the overall MSE for both the training set <i><b>and</b></i>
 the testing set. Before writing this article, I only tracked the MSE of
 the training set, without also tracking the MSE of the testing set. It 
was only when I sat down to write the article that I realized my 
mistake. So, when you see a discussion of MSE, it's the MSE of the 
training set and not the MSE of the testing set.</p>
<p>My first experimentations were designed to determine whether there 
was a need for distortions while training, or if they just got in the 
way. Over many epochs without distortions, I was never able to get the 
error rate in the testing set below around 140 mis-recognitions out of 
the 10,000 testing patterns, or a 1.40% error rate. To me this meant 
that distortions were needed, since without them, the neural network was
 not able to effectively generalize its learned behavior when confronted
 with a pattern (from the testing set) that it had never before seen.</p>
<p>I am still not able to explain why Dr. LeCun was able to achieve an 
error rate of 0.89% without distortions. Perhaps one day I will repeat 
these experiments, now that I have more experience in training the 
network.</p>
<p>Anyway, having determined that distortions were needed to improve 
training and generalization, my experimentations turned toward the 
selection of "good" values for distortions of the training patterns. Too
 much distortion seemed to prevent the neural network from reaching a 
stable point in its weights. In other words, if the distortions were 
large, then at the end of each epoch, the MSE for the training set 
remained very high, and did not reach a stable and small value. On the 
other hand, too little distortion prevented the neural network from 
generalizing its result to the testing set. In other words, if the 
distortion was small, then the results from the testing set were not 
satisfactory.</p>
<p>In the end, I selected values as follows. Note that the values of all tune-able parameters are found in the <i>.ini</i> file for the program:</p>
<ul>
<li>Maximum scale factor change (percent, like 20.0 for 20%) = 15.0 
</li><li>Maximum rotational change (degrees, like 20.0 for 20 degrees) = 15.0 
</li><li>Sigma for elastic distortions (higher numbers are more smooth and less distorted; Simard uses 4.0) = 8.0 
</li><li>Scaling for elastic distortions (higher numbers amplify distortions; Simard uses 0.34) = 0.5 </li></ul>&lt;!-- NOTE to me: Use file 30August2006-FourCycles-Another36Epochs-dot10MSE-94Errors.txt --&gt;
<p>Once these values were selected, I began to concentrate on the training progression, and on the influence of the learning rate <i>eta</i>.
 I set up a 24-hour run, starting from purely randomized weights, over 
which 30 epochs were completed. The training rate was varied from an 
initial value of 0.001 down to a minimum of 0.00001 over 20 epochs, and 
thereafter (i.e., for the final 10 epochs) was maintained at this 
minimum value. After training was completed, I ran the testing set to 
see how well the neural network performed.</p>
<p>The results were a very-respectable 1.14% error rate in the set of 
testing patterns. That is, in the 10,000 pattern testing set, of which 
the neural net had never before seen any of the patterns, and had never 
before even seen the handwriting of any of the writers, the neural 
network correctly recognized 9,886 patterns and incorrectly 
misrecognized only 114 patterns. For the set of training patterns, there
 were around 1880 mis-recognitions in the distorted patterns.</p>
<p>Naturally, although I was pleased that the neural network worked at 
all, I was not satisfied with this performance. Seven years earlier, Dr.
 LeCun had achieved an error rate of 0.82%. Certainly, given the passage
 of seven years, better results could be obtained.</p>
<p>So, I graphed backpropagation progress as a function of epoch. At each epoch, I graphed the learning rate <i>eta</i>,
 the MSE (of the training set), and the change in MSE relative to the 
previous epoch. Since the change in MSE bounced around a bit, I also 
graphed a smoothed version of delta MSE. Here are the results. Note that
 the right-hand axis is for MSE, whereas the left-hand axis is for 
everything else.</p>
<p><img alt="Backpropagation progress over 30 epochs starting from scratch" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/First-30-epochs.gif" border="0" height="337" width="592"> 
</p><p>The graph shows very clearly that the neural network reaches a 
constant MSE quickly, after around 12-15 epochs, and then does not 
improve any further. The asymptotic value of MSE was 0.135, 
corresponding to around 1880 mis-recognized patterns in the distorted 
training set. There was no significant improvement in the neural 
network's performance after 12-15 epochs, at least not for the learning 
rates that were used for the latter epochs.</p>
<p>My first reaction, given the asymptotic nature of the curve, was one 
of concern: maybe the neural network was incapable of learning any 
better than this. So, I restarted the training with the initial learning
 rate of 0.001. But this time, instead of starting from purely random 
weights, I used the weights that resulted from the first 30 epochs. 
After another 15 hours of training, here is the graph of results:</p>
<p><img alt="Backpropagation progress over another 25 epochs" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Second-25-epochs.gif" border="0" height="338" width="513"> 
</p><p>This graph was reassuring, since it showed that the neural 
network still had the capability to learn. It was simply a matter of 
teaching it correctly. At this point, the neural network had settled in 
on an MSE of around 0.113, corresponding to around 1550 mis-recognized 
patterns in the distorted training set. For the testing set, there were 
103 mis-recognized patterns out of 10,000, which is an error rate of 
1.03%. At least there was some improvement, which meant that the neural 
network could learn more, but not much improvement.</p>
<p>Looking at the graph, I decided that the learning rate should be set 
to a value that would allow the neural network to improve at a slow but 
deliberate pace. Arbitrarily, I tried a constant learning rate of 
0.00015, which (as seen in the above graph) should have allowed the 
neural network to improve around 0.01 MSE every three or so epochs. More
 specifically, looking at the "Smoothed Delta MSE" graph above, at a 
learning rate of around 0.00015, the delta MSE per epoch was around 
0.0033, which should have translated to an improvement of around 0.01 
MSE after every three epochs.</p>
<p>The results were miserable. As shown in the following graph, even 
after an additional 15 epochs at the carefully chosen constant learning 
rate of 0.00015, there was no noticeable improvement in the network. For
 the set of training patterns, there were still around 1550 
mis-recognized patterns; for the set of testing patterns, there was a 
slight improvement to 100 errors, for an error rate of exactly 1.00%:</p>
<p><img alt="Backpropagation progress over another 15 epochs at a constant learning rate of 0.0015" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Third-15-epochs.gif" border="0" height="338" width="512"> 
</p><p>I was not certain of why the neural network failed to improve. 
Clearly the learning rate was too small, given the small size of the 
errors being produced by the network, which after all, had decent 
performance. So, I decided to increase the learning rate to a much 
larger value. I abandoned the idea of trying to estimate the change in 
MSE as a function of learning rate, and arbitrarily chose a starting 
value of <i>eta</i> = 0.0005. It was also clear that the minimum 
learning rate of 0.00001, used previously, was too small; instead, I 
chose a minimum learning rate of 0.0002.</p>
<p>On the other hand, I thought that I might be rushing the neural 
network, in the sense that I had been decreasing the learning rate after
 each and every epoch. After all, the training patterns were distorted; 
as a consequence, the neural network never really saw the same training 
pattern twice, despite the fact that the same patterns were used for all
 epochs. I decided to let the neural network stick with the same 
training rate for four epochs in a row, before decreasing it. This 
produced good results:</p>
<p><img alt="Backpropagation progress over another 35 epochs with stepped learing rate" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Fourth-35-Stepped-Epochs.gif" border="0" height="339" width="556"> 
</p><p>As seen in the above graph, the neural network continued to learn
 for at least 15-20 epochs. The final MSE was somewhere around 0.102, 
corresponding to around 1410 mis-recognized characters in the distorted 
training set. On the testing set, there were only 94 errors (an error 
rate of 0.94%), so I had finally crossed the 1.00% threshold, and could 
see the possibility of meeting or exceeding the benchmark performance of
 Dr. LeCun's implementation.</p>
<p>But it had taken many, many epochs to get here. And each training 
cycle (except the first) had started with an already-trained network. I 
decided it was time to check whether the results could be reproduced, 
starting again from scratch, i.e., a blank neural network with purely 
random weights.</p>&lt;!-- Note to me: Use file 04September-SteppedFromScratch-64epochs-dot10MSE-86Errors.txt --&gt;
<p>I again chose an initial learning rate of 0.001, but in keeping with 
the experiences to date, chose a minimum learning rate of 0.00005 (which
 is five times larger than the minimum learning rate in the first 
experiment). I also used a stepped decrease in the learning rate, as 
above, where the learning rate was kept at the same value for 4 epochs 
before it was reduced. I frankly did not like the math when these 
parameters were multiplied together: It would take 52 epochs to reach 
the minimum learning rate, and at 40 minutes per epoch, that would 
require at least 35 hours of processing time. (Because of the timing of 
overnight runs, I actually let it run for 63 epochs, or 42 hours.) I bit
 the bullet, hit the "Backpropagate" button, and waited. The results 
showed something interesting:</p>
<p><img alt="Backpropagation progress over 63 epochs, starring from scratch, with a stepped learning rate" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Fifth-63-epochs-Stepped-From-Scratch.gif" border="0" height="336" width="597"> 
</p><p>First off, it should be apparent that the network trained itself 
well. The final MSE for the training set settled in at around 0.102, 
corresponding to around 1440 mis-recognitions in the distorted patterns 
of the training set. More importantly, mis-recognitions for the testing 
set were quite good: there were only 86 mis-recognitions out of the 
10,000 patterns in the testing set, for an error rate of 0.86%. Dr. 
LeCun's benchmark was in sight.</p>
<p>The interesting thing about the graph is the "bump" in the MSE that 
seems to appear after each time that the learning rate is decreased. 
Look carefully: there's a kind of periodicity to the "Delta MSE" curve, 
with a spike (which represents a good reduction in MSE) occurring right 
after the learning rate is decreased. I theorized that the neural 
network needs to see the same learning rate for more than one epoch, but
 does not need to see it for as many as four epochs. After more 
experimentation, I concluded that the learning rate needs to be constant
 for only two epochs. This is good news for training, since it means 
that the neural network can be trained, reproducibly from scratch, to 
sub-one-percent errors in less than around 15-18 hours.</p>
<p>All together, based on the above experimentation, these are the defaults that you see in the "Backpropagation" dialog:</p>
<ul>
<li>Initial learning rate (eta) = 0.001 
</li><li>Minimum learning rate (eta) = 0.00005 
</li><li>Rate of decay for learning rate (eta) = 0.794183335 
</li><li>Decay rate is applied after this number of backprops = 120000 </li></ul>
<p>The 0.86% error rate obtained above is very good, but it's still not 
as good as the 0.74% error rate in the title of this article. How did I 
get the error reduced further?</p>
<p>After some reflection on the results obtained so far, it appeared to 
me that there was still an apparent paradox in the error rates. On the 
training set, which the neural network actually sees, over and over 
again with only slight distortions, the network was not able to perform 
better than 1440 mis-recognitions out of 60,000 patterns, which works 
out to an error rate of 2.40%. That's nearly three times larger than the
 error rate of 0.86% for the testing set, which the neural network had 
never even seen. I theorized that while the distortions were clearly 
needed to force the neural network to generalize (remember: without 
distortions I was not able to achieve anything better than a 1.40% error
 rate on the testing set), after a while, they actually hampered the 
ability of the neural network to recognize and settle in on "good" 
weights. In a sense, the network might benefit from a final "polishing" 
with non-distorted patterns.</p>
<p>That's exactly what I did. First I trained (as above) with the 
distorted training set until I had obtained an error rate of 0.86% on 
the testing set. Then, I set the learning rate to a constant value of 
0.0001, which is small but is still twice as large as the minimum value 
of 0.00005. At this learning rate, I ran non-distorted training patterns
 for five "polishing" epochs. The selection of five epochs was somewhat 
arbitrary/intuitive: at five epochs, the neural network showed 272 
errors out of the 60,000 patterns in the training set, for an error rate
 of 0.45%. This value for error rate seemed appropriate, since it was 
smaller than the error rate on the testing set, yet not so small that 
all of the good generalizations introduced by distortions might be 
undone.</p>
<p>The "polishing" worked well, and resulted in the 0.74% error rate advertised above.</p>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Results"></a>
<h2>Results</h2>
<p>All 74 errors made by the neural network are shown in the following 
collage. For each pattern, the sequence number is given, together with 
an indication of the error made by the network. For example, "4176 
2=&gt;7" means that for pattern number 4176, the network misrecognized a
 2, and thought it was a 7:</p>
<p><img alt="Collage showing all 74 errors made by the neural network" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Collage-74-Errors.gif" border="0" height="600" width="429"> 
</p><p>For some of these patterns, it's easy to see why the neural 
network was wrong: the patterns are genuinely confusing. For example, 
pattern number 5937, which is purported to be a 5, looks to me like a 3,
 which is what the neural network thought too. Other patterns appear to 
have artifacts from pre-processing by the MNIST authors, and it's the 
presence of these artifacts that confused the network. For example, 
pattern number 5457, which is clearly a 1, also has artifacts on the 
right-hand side, possibly caused when the pattern was separated from an 
adjacent pattern. Other patterns, primarily in the 8's, have too much 
missing from the bottom of the patterns.</p>
<p>But for the vast majority, the neural network simply got it wrong. 
The patterns aren't especially confusing. Each of the 7's looks like a 
7, and there's no good reason why the neural network failed to recognize
 them as 7's.</p>
<p>Perhaps more training, with a larger training set, would help. I'm 
inclined to believe, however, that the network is at a point of 
diminishing returns with respect to training. In other words, more 
training might help some, but probably not much.</p>
<p>Instead, I believe that a different architecture is needed. For 
example, I think that the input layer should be designed with due 
recognition for the fact that the human visual system is very sensitive 
to spatial frequency. As a result, I believe that better performance 
could be obtained by somehow converting the input pattern to a frequency
 domain, which could be used to supplement the purely spatial input now 
being given as input. </p>
<p><a href="#topmost"><small>go back to top</small></a> <a name="Bibliography"></a>
</p><h2>Bibliography</h2>
<p>Here, in one place, is a list of all the articles and links mentioned
 in the article, as well as a few extra articles that might be helpful. 
Clicking the link will open a new window.</p>
<ul>
<li><a href="http://yann.lecun.com/exdb/publis/index.html" target="_newwin">List of publications by Dr. Yann LeCun</a> 
</li><li>Section of Dr. LeCun's web site on <a href="http://yann.lecun.com/exdb/lenet/index.html" target="_newwin">"Learning and Visual Perception"</a> 
</li><li>Microsoft's <a href="http://www.research.microsoft.com/dpu/" target="_newwin">"Document Processing and Understanding"</a> group 
</li><li><a href="http://research.microsoft.com/%7Epatrice/publi.html" target="_newwin">List of publications by Dr. Patrice Simard</a> 
</li><li><a href="http://www.nist.gov/srd/nistsd19.htm" target="_newwin">Database of handwritten patterns offered by the National Institute of Standards and Technology ("NIST")</a> 
</li><li><a href="http://yann.lecun.com/exdb/mnist/index.html" target="_newwin">Modified NIST ("MNIST") database (11,594 Kb total)</a> 
</li><li>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" target="_newwin">"Gradient-Based Learning Applied to Document Recognition,"</a> Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998. [46 pages] 
</li><li>Y. LeCun, L. Bottou, G. Orr, and K. Muller, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_newwin">"Efficient BackProp,"</a> in Neural Networks: Tricks of the trade, (G. Orr and Muller K., eds.), 1998. [44 pages] 
</li><li>Patrice Y. Simard, Dave Steinkraus, John Platt, <a href="http://research.microsoft.com/%7Epatrice/PDF/fugu9.pdf" target="_newwin">"Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis,"</a> International Conference on Document Analysis and Recognition (ICDAR), IEEE Computer Society, Los Alamitos, pp. 958-962, 2003. 
</li><li>Fabien Lauer, Ching Y. Suen and Gerard Bloch, <a href="http://hal.archives-ouvertes.fr/docs/00/05/75/61/PDF/LauerSuenBlochPR.pdf" target="_newwin">"A Trainable Feature Extractor for Handwritten Digit Recognition"</a>, Elsevier Science, February 2006 
</li><li><code>DlgResizeHelper</code> class, by Stephan Keil, described at <a href="http://www.codeguru.com/Cpp/W-D/dislog/resizabledialogs/article.php/c1913/" target="_newwin">Dialog Resize Helper</a> 
</li><li><a href="http://groups.google.com/group/comp.programming.threads/browse_thread/thread/1c3b38cd249ff2ba/e90ff2c919f84612" target="_newwin"><code>InterlockedCompareExchange64()</code> function</a> on the comp.programming.threads newsgroup </li></ul>
<p><a href="#topmost"><small>go back to top</small></a></p><a name="Version"></a>
<h2>License and Version Information</h2>
<p>The source code is licensed under the <a href="http://en.wikipedia.org/wiki/MIT_License" target="_blank">MIT X11-style open source license</a>.
 Basically, under this license, you can use/modify the code for almost 
anything you want. For a comparison with the BSD-style license and the 
much more restrictive GNU GPL-style license, read this Wikipedia 
article: <a href="http://en.wikipedia.org/wiki/BSD_and_GPL_licensing" target="_blank">"BSD and GPL licensing"</a>.</p>
<p>Version information:</p>
<ul>
<li>26 November 2006: Original release of the code and this article. </li></ul>
<p><a href="#topmost"><small>go back to top</small></a></p>


							</div>
							

							
							
							<h2>License</h2>
							<div id="LicenseTerms"><p>This article has no explicit license 
attached to it but may contain usage terms in the article text or the 
download files themselves. If in doubt please contact the author via the
 discussion board below.</p><p>A list of licenses authors might use can be found <a href="http://www.codeproject.com/info/Licenses.aspx">here</a></p></div>
							

							<div class="float-right" style="margin:20px 0 0 0;border:1px solid #ccc">
							<div class="msg-300x250" data-format="300x250" data-type="ad" data-publisher="lqm.codeproject.site" data-zone="ros" data-loadonview="true" data-tags="VC6, Win2K, WinXP, MFC, Dev, Advanced,rating4.5"><iframe id="dmad2" allowtransparency="false" style="z-index:10" marginwidth="0" marginheight="0" frameborder="0" height="250" scrolling="no" width="300"></iframe></div>
							</div>

							
							<h2 id="ctl00_AboutHeading">About the Author</h2>

							
							

<div class="container">
<div style="width:210px;overflow:hidden;float:left;text-align:center">
	<img id="ctl00_AboutAuthorRptr_ctl00_AboutAuthor_memberPhoto" class="profile-pic" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/F26CCF55-3B82-4EBF-8BE7-3F023319AFB5.jpg" style="border-width:0px;transform:rotate(2deg);">
</div>
<div class="container-member float-left" style="width:235px">
	<b><a id="ctl00_AboutAuthorRptr_ctl00_AboutAuthor_memberProfileLink" class="author" href="http://www.codeproject.com/Members/Mike-ONeill">Mike O'Neill</a></b>
	<div class="company">
		<span id="ctl00_AboutAuthorRptr_ctl00_AboutAuthor_memberJobTitle"></span>
		<span id="ctl00_AboutAuthorRptr_ctl00_AboutAuthor_memberCompany"></span> 
		<br><span id="ctl00_AboutAuthorRptr_ctl00_AboutAuthor_memberLocation">United States <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/US.gif" alt="United States" height="11px" width="16px"></span>
	</div>
</div>
	
<div class="padded-top float-left clearfix" style="width:600px">
	Mike O'Neill is a patent attorney in Southern California, where he 
specializes in computer and software-related patents.  He programs as a 
hobby, and in a vain attempt to keep up with and understand the 
technology of his clients.

	

	
</div>
</div><br>
							
							

							<div class="clearfix"></div>

							
							<div id="ctl00_RateArticleRow" class="clearfix voting-bar">
							<div class="float-left" style="padding-top:8px"><a class="anchorLink" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi#_articleTop">Article Top</a></div>
							<div class="float-left"><a id="_rating" name="_rating">&nbsp;</a></div> 
							<div class="float-left">

<div class="social-bookmarks">
<span class="facebook"><div class=" fb_reset" id="fb-root"><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div><iframe src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/xd_arbiter.html" style="border: medium none;" tab-index="-1" title="Facebook Cross Domain Communication Frame" aria-hidden="true" id="fb_xdm_frame_http" allowtransparency="true" name="fb_xdm_frame_http" frameborder="0" scrolling="no"></iframe><iframe src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/xd_arbiter_002.html" style="border: medium none;" tab-index="-1" title="Facebook Cross Domain Communication Frame" aria-hidden="true" id="fb_xdm_frame_https" allowtransparency="true" name="fb_xdm_frame_https" frameborder="0" scrolling="no"></iframe></div></div><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div></div></div></div><div fb-xfbml-state="rendered" class="fb-like fb_edge_widget_with_comment fb_iframe_widget" data-href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" data-send="false" data-layout="button_count" data-width="450" data-show-faces="false" data-font="segoe ui"><span style="height: 20px; width: 75px;"><iframe src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/like.html" class="fb_ltr" title="Like this content on Facebook." style="border: medium none; overflow: hidden; height: 20px; width: 75px;" name="f13a04ee55701de" id="f39ee2da39a7bf" scrolling="no"></iframe></span></div></span>
<span class="google" style="position:relative;top:-4px"><div id="___plusone_0" style="text-indent: 0px; margin: 0px; padding: 0px; background: none repeat scroll 0% 0% transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: baseline; display: inline-block; width: 90px; height: 20px;"><iframe title="+1" data-gapiattached="true" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/fastbutton.html" name="I0_1380056876280" id="I0_1380056876280" vspace="0" tabindex="0" style="position: static; top: 0px; width: 90px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 20px;" marginwidth="0" marginheight="0" hspace="0" frameborder="0" scrolling="no" width="100%"></iframe></div></span>
<span class="twitter" style="position:relative;top:-4px"><iframe data-twttr-rendered="true" title="Twitter Tweet Button" style="width: 106px; height: 20px;" class="twitter-share-button twitter-count-horizontal" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/tweet_button.html" allowtransparency="true" frameborder="0" scrolling="no"></iframe></span>
</div></div> 
							<div class="float-right align-right">
								<div id="ctl00_RateArticle_RateItemWrapper" class="small-text" name="RateItem_16650">

	<table class="small-text" cellpadding="0" cellspacing="0" width="100%">
	<tbody><tr>
		<td id="ctl00_RateArticle_VoteResultDiv" align="right" nowrap="nowrap">
			<span class="voteRes"></span>
			<img class="loaderImg" alt="loading..." src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/animated_loading_blue.gif" style="display:none;" height="16px" width="16px"> 
		</td>

	
		<td class="voteTbl" style="white-space:nowrap" align="right">
			<table class="small-text">
			<tbody><tr>
				<td id="ctl00_RateArticle_SignIn" nowrap="nowrap">
					<a href="#SignUp">Sign Up</a> to vote
				</td>

				
				<td id="ctl00_RateArticle_StartForm" align="right" nowrap="nowrap">
					<i>&nbsp;&nbsp;Poor</i>
				</td>

				<td id="ctl00_RateArticle_VoteFormDiv" class="nowrap">
					

					<span id="ctl00_RateArticle_RB" class="tooltip ajaxHist radio voting">
						<span id="ctl00_RateArticle_VoteRBL"><input id="ctl00_RateArticle_VoteRBL_0" name="ctl00$RateArticle$VoteRBL" value="1" onclick="ChkRtctl00_RateArticle(1, 16650);
$('#ctl00_RateArticle_RCD').show();
;" type="radio"><input id="ctl00_RateArticle_VoteRBL_1" name="ctl00$RateArticle$VoteRBL" value="2" onclick="ChkRtctl00_RateArticle(2, 16650);
$('#ctl00_RateArticle_RCD').show();
;" type="radio"><input id="ctl00_RateArticle_VoteRBL_2" name="ctl00$RateArticle$VoteRBL" value="3" onclick="ChkRtctl00_RateArticle(3, 16650);
$('#ctl00_RateArticle_RCD').show();
;" type="radio"><input id="ctl00_RateArticle_VoteRBL_3" name="ctl00$RateArticle$VoteRBL" value="4" onclick="ChkRtctl00_RateArticle(4, 16650);
$('#ctl00_RateArticle_RCD').show();
;" type="radio"><input id="ctl00_RateArticle_VoteRBL_4" name="ctl00$RateArticle$VoteRBL" value="5" onclick="ChkRtctl00_RateArticle(5, 16650);
$('#ctl00_RateArticle_RCD').show();
;" type="radio"></span> 

						
					</span>

				</td>

				<td id="ctl00_RateArticle_EndForm" align="left">
					<i>Excellent</i>
				</td>

				<td style="padding-left:5px">	
					<input name="ctl00$RateArticle$SubmitRateBtn" value="Vote" onclick="return PostBack_ctl00_RateArticle_RateItemWrapper();" id="ctl00_RateArticle_SubmitRateBtn" class="button" type="submit">
				</td>
			</tr>
			</tbody></table>
			
		</td>
	</tr>
	</tbody></table>
	<div class="hover-container">
		<div id="ctl00_RateArticle_RCD" class="rating-comment align-left float-right">
			Add a reason or comment to your vote: <a href="#" id="clear-rate_ctl00_RateArticle_RCD" title="close">x</a><br>
			<textarea class="RateComment" rows="5" cols="60" style="width:98%;"></textarea>
			<span id="ctl00_RateArticle_CommentReq" class="subdue">Votes of 3 or less require a comment</span>
		</div>
	</div>
</div>
							</div>
							</div>
							

						</form>

						
						<div style="margin:auto;height:90px;margin-top:10px"> 
							<div class="msg-728x90" data-format="728x90" data-type="ad" data-publisher="lqm.codeproject.site" data-zone="bottom" data-loadonview="true" data-tags="VC6, Win2K, WinXP, MFC, Dev, Advanced,pos_bottom"><iframe id="dmad3" allowtransparency="false" style="z-index:10" marginwidth="0" marginheight="0" frameborder="0" height="90" scrolling="no" width="728"></iframe></div>
						</div>
						
					

				</div>

				
					
					<h2>Comments and Discussions</h2>
					<a class="float-left" name="_comments" id="_comments">&nbsp;</a><table id="ForumTable" class="forum relaxed" cellpadding="0" cellspacing="0">
	<tbody><tr>
		<td class="header1 callout"><b>Hint:</b> For improved responsiveness ensure Javascript is enabled and choose 'Normal' from the Layout dropdown and hit 'Update'.<br><b>You must <a href="https://www.codeproject.com/script/Membership/LogOn.aspx?rp=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi%3ffid%3d364895">Sign In</a> to use this message board.</b></td>
	</tr><tr>
		<td><table border="0" cellpadding="3px" cellspacing="0" width="100%">
			<tbody><tr class="header1">
				<td colspan="2" style="white-space:nowrap;"><div class="container">
					<div class="float-right">
						<form action="/Search.aspx?fid=0" method="get" class="tight">
							<input name="fid" value="364895" type="hidden"><b>Search this forum </b><input class="text-input" name="qf" style="width:200px;" type="text">&nbsp;<input value="Go" class="button" type="submit">
						</form>
					</div>
				</div></td></tr><tr class="header2">
					<td></td><td style="width:100%;"><div style="text-align:right;">
						<form action="/script/Forums/SetOptions.aspx?floc=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi&amp;fid=364895" method="get" style="margin:0;padding:0;">
							<input name="fid" value="364895" type="hidden"><input name="currentQS" value="?floc=%2fArticles%2f16650%2fNeural-Network-for-Recognition-of-Handwritten-Digi&amp;fid=364895" type="hidden"><input name="floc" value="/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi" type="hidden"><input checked="checked" name="prof" id="prof" style="vertical-align:middle;" type="checkbox"><label for="prof">Profile popups</label>&nbsp;&nbsp;&nbsp;&nbsp;Spacing<select size="1" class="dropdown" name="spc">
								<option selected="selected" value="Relaxed">Relaxed</option><option value="Compact">Compact</option><option value="Tight">Tight</option>
							</select>&nbsp;&nbsp;Noise<select size="1" class="dropdown" name="noise">
								<option value="1">Very High</option><option value="2">High</option><option selected="selected" value="3">Medium</option><option value="4">Low</option><option value="5">Very Low</option>
							</select>&nbsp;&nbsp;Layout<select size="1" class="dropdown" name="view">
								<option selected="selected" value="Quick">Normal</option><option value="Topic">Open Topics</option><option value="Expanded">Open All</option><option value="Thread">Thread View</option><option value="Normal">No Javascript</option><option value="Preview">Preview</option>
							</select>&nbsp;&nbsp;Per page<select size="1" class="dropdown" name="mpp">
								<option value="10">10</option><option selected="selected" value="25">25</option><option value="50">50</option>
							</select>&nbsp;&nbsp;&nbsp;<input value="Update" name="SetOpt" class="button" type="submit">
						</form>
					</div></td>
				</tr>
			
		</tbody></table></td></tr><tr>
			<td><a name="xx0xx"></a><table border="0" cellpadding="2px" cellspacing="0" width="100%">
				<tbody><tr class="navbar">
					<td></td><td style="text-align:right;width:50%;"></td><td style="text-align:right;white-space:nowrap;"><span class="nav-link disabled">First</span> <span class="nav-link disabled">Prev</span><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=26#xx0xx">Next</a></td>
				</tr>
			</tbody></table></td>
		</tr><tr>
			<td><table class="fixed-layout blank-background" border="0" cellpadding="0" cellspacing="0" width="100%">
				<tbody><tr>
					<td><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/t.gif" alt="" border="0" height="5px" width="1px"></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4633096xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4633096" parent="0" thread="4633096" href="http://www.codeproject.com/Messages/4633096/want-the-whole-code.aspx">want   the whole code</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=10197013">zerotwoyy</a></td><td class="date">9-Aug-13  22:15&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4577759xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4577759" parent="0" thread="4577759" href="http://www.codeproject.com/Messages/4577759/My-vote-of-5.aspx">My vote of 5</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=10085837">new_coder2013</a></td><td class="date">1-Jun-13  19:57&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4559945xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4559945" parent="0" thread="4559945" href="http://www.codeproject.com/Messages/4559945/Can-that-architecture-be-used-for-letter-recogniti.aspx">Can that architecture be used for letter recognition?</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=10037784">Member 10037784</a></td><td class="date">8-May-13  7:57&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4539867xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4539867" parent="0" thread="4539867" href="http://www.codeproject.com/Messages/4539867/Help-Im-stuck.aspx">Help! I'm stuck</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9984880">BnVn101</a></td><td class="date">12-Apr-13  22:53&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4504419xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4504419" parent="0" thread="4504419" href="http://www.codeproject.com/Messages/4504419/Activation-Function.aspx">Activation Function</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=681013">OnlyJoe</a></td><td class="date">25-Feb-13  10:15&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4503768xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4503768" parent="0" thread="4503768" href="http://www.codeproject.com/Messages/4503768/CNN-Structure.aspx">CNN Structure</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9701709">Anurag Gupta</a></td><td class="date">24-Feb-13  20:54&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4399313xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4399313" parent="0" thread="4399313" href="http://www.codeproject.com/Messages/4399313/Question-about-backpropagation-training.aspx">Question about backpropagation training</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9505654">xiaoleih</a></td><td class="date">16-Oct-12  4:03&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4377685xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4377685" parent="0" thread="4377685" href="http://www.codeproject.com/Messages/4377685/regarding-increase-in-input-size.aspx">regarding increase in input size</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=7060129">ATISH VAZE</a></td><td class="date">24-Sep-12  20:42&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="38px"><a name="xx4399320xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_answer.gif" alt="Answer" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4399320" parent="4377685" thread="4377685" href="http://www.codeproject.com/Messages/4399320/Re-regarding-increase-in-input-size.aspx">Re: regarding increase in input size</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9505654">xiaoleih</a></td><td class="date">16-Oct-12  4:06&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4336160xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4336160" parent="0" thread="4336160" href="http://www.codeproject.com/Messages/4336160/whether-there-is-a-problem-in-your-code.aspx">whether there is a problem in your code</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=6526260">chenrui6321278</a></td><td class="date">11-Aug-12  21:43&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="38px"><a name="xx4339633xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_answer.gif" alt="Answer" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4339633" parent="4336160" thread="4336160" href="http://www.codeproject.com/Messages/4339633/Re-whether-there-is-a-problem-in-your-code-Yes-Re-.aspx">Re: whether there is a problem in your code: Yes, Re-Confirmed Again</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=208786">Mike O'Neill</a></td><td class="date">15-Aug-12  10:44&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="56px"><a name="xx4339944xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4339944" parent="4339633" thread="4336160" href="http://www.codeproject.com/Messages/4339944/Re-whether-there-is-a-problem-in-your-code-Yes-Re-.aspx">Re: whether there is a problem in your code: Yes, Re-Confirmed Again</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=6526260">chenrui6321278</a></td><td class="date">15-Aug-12  22:09&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4290131xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4290131" parent="0" thread="4290131" href="http://www.codeproject.com/Messages/4290131/My-vote-of-5.aspx">My vote of 5</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9128876">Jerry Jin</a></td><td class="date">24-Jun-12  17:08&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4289899xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4289899" parent="0" thread="4289899" href="http://www.codeproject.com/Messages/4289899/Need-help-with-math-equations-in-this-article.aspx">Need help with math equations in this article</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9128876">Jerry Jin</a></td><td class="date">24-Jun-12  1:33&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="38px"><a name="xx4290007xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_answer.gif" alt="Answer" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4290007" parent="4289899" thread="4289899" href="http://www.codeproject.com/Messages/4290007/Re-Need-help-with-math-equations-in-this-article.aspx">Re: Need help with math equations in this article</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=208786">Mike O'Neill</a></td><td class="date">24-Jun-12  7:43&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="56px"><a name="xx4290130xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4290130" parent="4290007" thread="4289899" href="http://www.codeproject.com/Messages/4290130/Re-Need-help-with-math-equations-in-this-article.aspx">Re: Need help with math equations in this article</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=9128876">Jerry Jin</a></td><td class="date">24-Jun-12  17:03&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4225250xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4225250" parent="0" thread="4225250" href="http://www.codeproject.com/Messages/4225250/My-vote-of-5.aspx">My vote of 5</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=3318782">Vietdungiitb</a></td><td class="date">18-Apr-12  17:33&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4217856xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4217856" parent="0" thread="4217856" href="http://www.codeproject.com/Messages/4217856/My-vote-of-5.aspx">My vote of 5</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=5071129">manoj kumar choubey</a></td><td class="date">11-Apr-12  3:37&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4212667xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4212667" parent="0" thread="4212667" href="http://www.codeproject.com/Messages/4212667/very-nice-job.aspx">very nice job</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=3167738">Member 3167738</a></td><td class="date">4-Apr-12  19:20&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4192881xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4192881" parent="0" thread="4192881" href="http://www.codeproject.com/Messages/4192881/My-vote-of-5.aspx">My vote of 5</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=4320844">Member 4320844</a></td><td class="date">16-Mar-12  4:53&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="38px"><a name="xx4195001xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4195001" parent="4192881" thread="4192881" href="http://www.codeproject.com/Messages/4195001/Re-My-vote-of-5.aspx">Re: My vote of 5</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=208786">Mike O'Neill</a></td><td class="date">19-Mar-12  9:59&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx4056639xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="4056639" parent="0" thread="4056639" href="http://www.codeproject.com/Messages/4056639/About-the-number-of-possible-positions-of-the-conv.aspx">About the number of possible positions of the convolution kernel [modified]</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=6675850">yacek1234</a></td><td class="date">19-Oct-11  6:03&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx3949347xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="3949347" parent="0" thread="3949347" href="http://www.codeproject.com/Messages/3949347/How-reduce-the-number-of-patterns.aspx">How reduce the number of patterns</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=8042529">gabrieldarge</a></td><td class="date">5-Jul-11  23:59&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row root">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="20px"><a name="xx3883526xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="3883526" parent="0" thread="3883526" href="http://www.codeproject.com/Messages/3883526/Learning-Rate-Problem-modified.aspx">Learning Rate Problem [modified]</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=5275606">kebomix</a></td><td class="date">6-May-11  19:35&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr class="header hover-row">
					<td class="subject-line none " width="100%"><table border="0" cellpadding="0" cellspacing="0" width="100%">
						<tbody><tr>
							<td class="indent" width="38px"><a name="xx3885876xx"></a><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"></td><td class="subject hover-container"><a class="message-link" name="3885876" parent="3883526" thread="3883526" href="http://www.codeproject.com/Messages/3885876/Re-Learning-Rate-Problem.aspx">Re: Learning Rate Problem</a></td><td class="icon"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/icn-member-16.gif" title="member" alt="member" border="0" height="16px"></td><td class="author"><a href="http://www.codeproject.com/script/Membership/View.aspx?mid=208786">Mike O'Neill</a></td><td class="date">9-May-11  14:01&nbsp;</td>
						</tr>
					</tbody></table></td>
				</tr><tr>
					<td><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/t.gif" alt="" border="0" height="5px" width="1px"></td>
				</tr>
			</tbody></table></td>
		</tr><tr>
			<td><table cellpadding="2px" cellspacing="0" width="100%">
				<tbody><tr class="footer">
					<td>Last Visit: 31-Dec-99  18:00 &nbsp; &nbsp; Last Update: 24-Sep-13  7:08</td><td><a href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895">Refresh</a></td><td style="text-align:right;white-space:nowrap;"><input id="_mbnUrl" value="/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=26" type="hidden"><span class="nav-link selected">1</span><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=26#xx0xx">2</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=51#xx0xx">3</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=76#xx0xx">4</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=101#xx0xx">5</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=126#xx0xx">6</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=151#xx0xx">7</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=176#xx0xx">8</a><a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=201#xx0xx">9</a> <a class="nav-link" name="Frm_HoverNL" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?fid=364895&amp;fr=26#xx0xx">Next »</a></td>
				</tr>
			</tbody></table></td>
		</tr>
	
</tbody></table><p class="small-text"><img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_general.gif" alt="General" align="top" height="16px" width="16px"> General &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_news.gif" alt="News" align="top" height="16px" width="16px"> News &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_idea.gif" alt="Suggestion" align="top" height="16px" width="16px"> Suggestion &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_question.gif" alt="Question" align="top" height="16px" width="16px"> Question &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_bug.gif" alt="Bug" align="top" height="16px" width="16px"> Bug &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_answer.gif" alt="Answer" align="top" height="16px" width="16px"> Answer &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_joke.gif" alt="Joke" align="top" height="16px" width="16px"> Joke &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_rant.gif" alt="Rant" align="top" height="16px" width="16px"> Rant &nbsp;&nbsp; <img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/msg_admin.gif" alt="Admin" align="top" height="16px" width="16px"> Admin &nbsp;&nbsp; </p>

				

			</div>
			
		</td>
		<td>
			<div id="ctl00_RightSideBar" class="container-article-info">

				<div class="header">About Article</div>
				<div class="article-summary">

					
					

					
					

					
					<div class="summary"><span id="ctl00_ArticleDescr">A convolutional neural network achieves 99.26% accuracy on a modified NIST database of hand-written digits.</span></div>

					<a id="ctl00_InfoBox_ParentLink"></a>

<table class="article-info" cellpadding="0" cellspacing="0">

	
	<tbody><tr><td>Type&nbsp;</td><td class="value"><a id="ctl00_InfoBox_TypeName" href="http://www.codeproject.com/script/Articles/Types.aspx?#Article">Article</a></td></tr>
	

	<tr><td>Licence&nbsp;</td><td class="value"></td></tr>

	

	<tr><td>First Posted&nbsp;</td><td class="value" nowrap="nowrap"><span itemprop="datePublished" content="2006-12-05">5 Dec 2006</span></td></tr>

	<tr><td>Views&nbsp;</td><td class="value">445,428</td></tr>

		

	
	<tr><td>Bookmarked&nbsp;</td><td class="value">476 times</td></tr>
	

	

	
	
	
	<tr><td colspan="2"></td>
	</tr>
	
	
</tbody></table>

					<div class="tags"> 
					<span id="ctl00_TagsList_TagWrp" class="tags">
	
	
	
	<span id="ctl00_TagsList_VisibleTags"><span class="t"><a rel="tag" href="http://www.codeproject.com/Tags/VC6">VC6</a></span><span class="t"><a rel="tag" href="http://www.codeproject.com/Tags/Win2K">Win2K</a></span><span class="t"><a rel="tag" href="http://www.codeproject.com/Tags/WinXP">WinXP</a></span><span class="t"><a rel="tag" href="http://www.codeproject.com/Tags/MFC">MFC</a></span><span class="t"><a rel="tag" href="http://www.codeproject.com/Tags/Dev">Dev</a></span><br><span class="t"><a rel="tag" href="http://www.codeproject.com/Tags/Advanced">Advanced</a></span></span> 

	
	
</span>

					</div>

					<div class="nowrap align-left">
						 

<a id="ctl00_ActionLinks_PrintLnk" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?display=Print">
	<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/print.gif" style="border:0" height="46px" width="46px">
</a>

<a id="ctl00_ActionLinks_MailLink" href="http://www.codeproject.com/script/common/TellFriend.aspx?obtid=2&amp;obid=16650">
	<img src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/email.gif" style="border:0" height="46px" width="46px">
</a>
					</div>

					
				</div>

				<div style="width:160px;margin: 10px auto;">
					<div class="msg-160x600" data-format="160x600" data-type="ad" data-publisher="lqm.codeproject.site" data-zone="ros" data-tags="VC6, Win2K, WinXP, MFC, Dev, Advanced,rating4.5"><iframe id="dmad4" allowtransparency="false" style="z-index:10" marginwidth="0" marginheight="0" frameborder="0" height="600" scrolling="no" width="160"></iframe></div>
				</div>

				
<div class="padded-top">
<div class="header">Top News</div>

<p><a id="ctl00_News_News_ctl01_Link" href="http://blog.splinter.me/salaries-for-developers-and-designers-across-the-world/">Salaries for Developers and Designers across the world</a></p>

Get the <a id="ctl00_News_News_ctl02_Subscribe" href="http://www.codeproject.com/Feature/Insider/">Insider News</a> free each morning.
</div>

				

				<div class="padded-top">
					
	<div id="ctl00_RelatedVideos_RelatedResults_ctl00_header" class="header">Related Videos</div>
	<div class="content-list align-center">	
	
	<div class="content-list-item">
		<a id="ctl00_RelatedVideos_RelatedResults_ctl01_Link" class="title" href="http://codeproject.tv/video/5000855/javascript_chrome_developer_tools_network_panel"><img id="ctl00_RelatedVideos_RelatedResults_ctl01_Thumbnail" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/cptv100x80ms.jpg" style="border-style:None;width:125px;border-width:0px;"></a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedVideos_RelatedResults_ctl02_Link" class="title" href="http://codeproject.tv/video/4938430/working_with_gestures"><img id="ctl00_RelatedVideos_RelatedResults_ctl02_Thumbnail" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/ios-Thumbnail.png" style="border-style:None;width:125px;border-width:0px;"></a>
		
	</div>
	
	</div>
	

				</div>
				<div class="padded-top">
					
	<div id="ctl00_RelatedArticles_RelatedResults_ctl00_header" class="header">Related Articles</div>
	<div class="content-list">	
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl01_Link" class="title" href="http://www.codeproject.com/Articles/523074/Online-handwriting-recognition-using-multi-convolu">Online handwriting recognition using multi convolution neural networks</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl02_Link" class="title" href="http://www.codeproject.com/Articles/376798/Large-pattern-recognition-system-using-multi-neura">Large pattern recognition system using multi neural networks</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl03_Link" class="title" href="http://www.codeproject.com/Articles/24361/A-Neural-Network-on-GPU">A Neural Network on GPU</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl04_Link" class="title" href="http://www.codeproject.com/Articles/571462/Multiple-convolution-neural-networks-approach-for-">Multiple convolution neural networks approach for online handwriting recognition</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl06_Link" class="title" href="http://www.codeproject.com/Articles/143059/Neural-Network-for-Recognition-of-Handwritten-Digi">Neural Network for Recognition of Handwritten Digits in C#</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl07_Link" class="title" href="http://www.codeproject.com/Articles/74348/Handwriting-Recognition-using-Kernel-Discriminant-">Handwriting Recognition using Kernel Discriminant Analysis</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl08_Link" class="title" href="http://www.codeproject.com/Articles/140631/Convolutional-Neural-Network-Workbench">Convolutional Neural Network Workbench</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl09_Link" class="title" href="http://www.codeproject.com/Articles/106583/Handwriting-Recognition-Revisited-Kernel-Support-V">Handwriting Recognition Revisited: Kernel Support Vector Machines</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl10_Link" class="title" href="http://www.codeproject.com/Articles/15304/Unicode-Optical-Character-Recognition">Unicode Optical Character Recognition</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl11_Link" class="title" href="http://www.codeproject.com/Articles/14188/Brainnet-1-A-Neural-Netwok-Project-With-Illustrati">Brainnet
 1 - A Neural Netwok Project - With Illustration And Code - Learn Neural
 Network Programming Step By Step And Develop a Simple Handwriting 
Detection System</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl12_Link" class="title" href="http://www.codeproject.com/Articles/346244/UPV-UNIPEN-online-handwriting-recognition-database">UPV – UNIPEN online handwriting recognition database viewer control</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl13_Link" class="title" href="http://www.codeproject.com/Articles/16447/Neural-Networks-on-Csharp">Neural Networks on C#</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl14_Link" class="title" href="http://www.codeproject.com/Articles/3907/Creating-Optical-Character-Recognition-OCR-applica">Creating Optical Character Recognition (OCR) applications using Neural Networks</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl15_Link" class="title" href="http://www.codeproject.com/Articles/1591/Mouse-gestures-recognition">Mouse gestures recognition</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl16_Link" class="title" href="http://www.codeproject.com/Articles/363596/Library-for-online-handwriting-recognition-system-">Library for online handwriting recognition system using UNIPEN database.</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl17_Link" class="title" href="http://www.codeproject.com/Articles/160868/A-Csharp-Project-in-Optical-Character-Recognition-">A C# Project in Optical Character Recognition (OCR) Using Chain Code</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl18_Link" class="title" href="http://www.codeproject.com/Articles/11285/Neural-Network-OCR">Neural Network OCR</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl19_Link" class="title" href="http://www.codeproject.com/Articles/477689/JavaScript-Machine-Learning-and-Neural-Networks-wi">JavaScript Machine Learning and Neural Networks with Encog</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl20_Link" class="title" href="http://www.codeproject.com/Articles/19323/Image-Recognition-with-Neural-Networks">Image Recognition with Neural Networks</a>
		
	</div>
	
	<div class="content-list-item">
		<a id="ctl00_RelatedArticles_RelatedResults_ctl21_Link" class="title" href="http://www.codeproject.com/Articles/54575/An-Introduction-to-Encog-Neural-Networks-for-Cshar">An Introduction to Encog Neural Networks for C#</a>
		
	</div>
	
	</div>
	

				</div>
				<div class="padded-top">
					
				</div>

				
				
			</div>
		</td>
		</tr></tbody></table>

		
		<div class="theme1-background" style="height:2px"></div>

		<div class="extended tiny-text">
			<div class="row">
				<div class="float-left">
					<a id="ctl00_PermaLink" itemprop="url" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi">Permalink</a> | 
					<a id="ctl00_AdvertiseLink" href="http://developermedia.com/">Advertise </a> |
					<a id="ctl00_PrivacyLink" href="http://www.codeproject.com/info/privacy.aspx">Privacy</a> |
					<a id="ctl00_Mobile" rel="nofollow" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?display=Mobile">Mobile</a>
					<br>
								
					
					Web01 |
					2.6.1309023.2 |
					Last Updated 5 Dec 2006								
				</div>
				<div class="float-right align-right">
					Article Copyright 2006 by Mike O'Neill<br>Everything else
					Copyright © <a href="mailto:webmaster@codeproject.com">CodeProject</a>, 1999-2013 <br>
					<a id="ctl00_TermsOfUseLink" href="http://www.codeproject.com/info/TermsOfUse.aspx">Terms of Use</a>
				</div>

				


<div class="page-width">
Layout: <a id="ctl00_PageWidth_FixedT" title="Fixed width layout" rel="nofollow" class=" active" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?PageFlow=FixedWidth">fixed</a>
|
<a id="ctl00_PageWidth_FluidT" title="Fluid layout" rel="nofollow" href="http://www.codeproject.com/Articles/16650/Neural-Network-for-Recognition-of-Handwritten-Digi?PageFlow=Fluid">fluid</a>
</div>



			</div>
		</div>
		

		<br clear="all">
		
			

	</div> 
	</div>
</div>


<div style="display:none;" id="lqm_AdTable">
	
</div>


<script type="text/javascript" language="Javascript" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/jquery.js"></script><script type="text/javascript">//<![CDATA[
if (typeof jQuery == 'undefined') {
    document.write(unescape("%3Cscript src='/script/JS/jquery-1.6.2.min.js' type='text/javascript' %3E%3C/script%3E"));
}//]]></script>
<script type="text/javascript" language="Javascript" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/article.js"></script>
<script type="text/javascript" language="Javascript" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/navbar.js"></script>
<script type="text/javascript" language="Javascript" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/Notifications.js"></script>
<script type="text/javascript" language="Javascript" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/MemberProfilePopup.js"></script>
<script type="text/javascript" language="Javascript">//<![CDATA[
$(document).ready(function() { anchorAnimate(); });
(function(d, s, id) { var js, fjs = d.getElementsByTagName(s)[0]; if (d.getElementById(id)) return; js = d.createElement(s); js.id = id; js.src = "//connect.facebook.net/en_GB/all.js#xfbml=1"; fjs.parentNode.insertBefore(js, fjs); }(document, 'script', 'facebook-jssdk'));
(function() {var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = 'https://apis.google.com/js/plusone.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s); })();
!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){ js=d.createElement(s);js.id=id; js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");
var DMReportsOK = null;function showDMAlert() {   var $div = $('div.msg-728x90');  $div.append($('<img src="/images/alert-top-block.gif" style="right:0;position:absolute;z-index:0">'));}function onDMcallBack(event){   if (event.originalEvent) event = event.originalEvent;   jQuery.support.cors = true;   if (event.data == 'DM-disabled') DMReportsOK = false;   else if (event.data == 'DM-enabled') DMReportsOK = true;}function checkLoaded() {  var recordCheck  = false; var showBlockMsg = false; var block        = 'None'; if (!DMReportsOK && block == 'None' && typeof DMAds === 'undefined')     block = 'ResourceBlocked'; if (DMReportsOK === true) block = 'None'; else if (DMReportsOK === false) block = 'DomModified'; if (recordCheck) {  $.ajax({ url:'/script/AdServe/Ajax/VS.aspx',data:{'adbm':block}, cache:false, async:true }); } if (showBlockMsg && block != 'None')    showDMAlert();}$(document).ready(function() {  $(window).bind('message', onDMcallBack);  setTimeout(checkLoaded, 4000);});
$(function ()
                {
                    $('.oauth').click(function () {
                        $this = $(this);
                        href = $this.attr('href');
                        var myWindow = window.open(href, 'popup',
                                    'width=800,height=600,location=0,menubar=0,resizeable=0,scrollbars=0,toolbar=0');
                        myWindow.focus();
                        var timer = setInterval(function () {
                                        if (myWindow.closed) {
                                            clearInterval(timer);
                                            // window.location.reload(); // May do a POST reload, shows a warning
                                            window.location = window.location; // force a GET reload
                                        }
                                    }, 200);
                        return false;
                    });
                });
var oSrchFlt = false, oSrchBox=false,srchBoxFoc=false;
$(document).ready(function() {
 if(InitWatermark)InitWatermark('sb_tb', 'Search for articles, questions, tips');
 var sbar = $('#sb_tb'); 
 var sfilter = $('#SearchFilter');
 if (sbar && sfilter) {
  sfilter.removeClass('popup'); sfilter.hide();
  sbar.blur(function() {
 if (!oSrchFlt)sfilter.hide();
 srchBoxFoc=false;
 /*sbar.animate( { width:'210px' }, { queue:false, duration:300 });
*/ });
  sbar.focus(function() {
 oSrchFlt=false;srchBoxFoc=true;
 sfilter.show();
 /*sbar.animate( { width:'500px' }, { queue:false, duration:300 });
*/ });
  sbar.mouseleave(function() { oSrchBox=false; });
  sbar.mouseover(function() { oSrchBox=true; });
  sfilter.mouseleave(function() { oSrchFlt=false; if (!srchBoxFoc&&!oSrchBox)sfilter.hide();});
  sfilter.mouseover(function() { oSrchFlt=true; });
 }
});
$("#ctl00_RateArticle_RateItemWrapper").removeClass("container-rating");$('#clear-rate_ctl00_RateArticle_RCD').click(function () { $('#ctl00_RateArticle_RCD').hide(); return false;});
function PostBack_ctl00_RateArticle_RateItemWrapper() {
  return rateItem(16650,2,1,true,true,3,'LargeStars');
}
function ChkRtctl00_RateArticle(val, objId) {if (val<=3||true) {
$('div[name=RateItem_' + objId + '] .rating-comment').css("display","");}
else $('div[name=RateItem_' + objId + '] .rating-comment').css("display","none");}

forumDir = '/script/Forums/';
staticServer = '';
allowReporting = false;
allowRating = false;
allowRatingDisplay = true;
var smoothScroll = false;
Selected        = -1;
oldTitle        = document.title;
minMessageScore = 1;
minMessageScore = 5;
abuseScore      = -2;
spamScore       = -1;
getRatingUrl    = '/script/Ratings/Ajax/GetRatings.aspx';
noiseThreshold  = 3;
getRatingRefKey = 'obrs';


//]]>
</script>



<canvas id="cv1" width="1px" height="1px" style="position:absolute;left:0;top:0;pointer-events:none"></canvas><canvas id="cv2" width="1px" height="1px" style="position:absolute;left:0;top:0;pointer-events:none"></canvas><div style="display: none; position: absolute;" id="MemberProfilePopupDiv" class="raised box"></div><div style="display: none; position: absolute;" id="MemberProfilePopupDiv" class="raised box"></div><div style="display: none; position: absolute;" id="MemberProfilePopupDiv" class="raised box"></div><iframe style="width: 1px; height: 1px; position: absolute; top: -100px;" src="Neural%20Network%20for%20Recognition%20of%20Handwritten%20Digits%20-%20CodeProject_files/postmessageRelay.html" id="oauth2relay550295270" name="oauth2relay550295270"></iframe></body></html>